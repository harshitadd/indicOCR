{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "VisionOCR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/AI4BharatTranslation/blob/main/VisionOCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElNyHm_cUXJ4"
      },
      "source": [
        "import glob\n",
        "import uuid\n",
        "import json\n",
        "import requests\n",
        "import copy\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "import logging"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjsH9HKyUXKG"
      },
      "source": [
        "path = '/content/'\n",
        "output_path = '/content/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oySRhhM-UXKH"
      },
      "source": [
        "language = 'detect'\n",
        "file_format = 'PDF'\n",
        "\n",
        "evaluation_level = 'LINE'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouaxf0LlUXKJ"
      },
      "source": [
        "token = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyTmFtZSI6ImhhcnNoaXRhZGRAZ21haWwuY29tIiwicGFzc3dvcmQiOiJiJyQyYiQxMiR2L25RbDQ0dWZHdXhoUEJpZElHYm11S2hMcHVWLmhwQWs5cTF6M2UvNkoueXltUEJnUFNkTyciLCJleHAiOjE2MTQ0MDAzNDN9.eNtXbLuufgaU3q69mUSDxSjXmmcs3FO9s31VurJ5TIE'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj6xomfsUXKK"
      },
      "source": [
        "#path = '/home/srihari/Desktop/data/data'\n",
        "word_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "google_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "layout_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "segmenter_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "bs_url =\"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/jobs/search/bulk\"\n",
        "\n",
        "evaluator_url  = \"https://auth.anuvaad.org/anuvaad-etl/document-processor/evaluator/v0/process\"\n",
        "\n",
        "#evaluator_url = 'http://0.0.0.0:5001/anuvaad-etl/document-processor/evaluator/v0/process'\n",
        "\n",
        "download_url =\"https://auth.anuvaad.org/download/\"\n",
        "upload_url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "\n",
        "\n",
        "headers = {\n",
        "    'auth-token' :token }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4LNV27mUXKL"
      },
      "source": [
        "class Draw:\n",
        "    \n",
        "    def __init__(self,input_json,save_dir,regions,prefix='',color= (255,0,0),thickness=5):   \n",
        "        self.json = input_json\n",
        "        self.save_dir = save_dir\n",
        "        self.regions = regions\n",
        "        self.prefix  = prefix\n",
        "        self.color  = color\n",
        "        self.thickness=thickness\n",
        "        if self.prefix in ['seg','tess']:\n",
        "            #print('drawing children')\n",
        "            self.draw_region_children()\n",
        "        else:\n",
        "            self.draw_region()\n",
        "        \n",
        "    def get_coords(self,page_index):\n",
        "        return self.json['outputs'][0]['pages'][page_index][self.regions]\n",
        "    \n",
        "    def get_page_count(self):\n",
        "        return(self.json['outputs'][0]['page_info'])\n",
        "    \n",
        "    def get_page(self,page_index):\n",
        "        page_path = self.json['outputs'][0]['page_info'][page_index]\n",
        "        page_path = page_path.split('upload')[1]#'/'.join(page_path.split('/')[1:])\n",
        "        print(page_path)    \n",
        "        return download_file(download_url,headers,page_path,f_type='image')\n",
        "\n",
        "    def draw_region(self):\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "        for page_index in range(len(self.get_page_count())) :\n",
        "            nparr = np.fromstring(self.get_page(page_index), np.uint8)\n",
        "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "            for region in self.get_coords(page_index) :\n",
        "                    ground = region['boundingBox']['vertices']\n",
        "                    pts = []\n",
        "                    for pt in ground:\n",
        "                        pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "                    cv2.polylines(image, [np.array(pts)],True, self.color, self.thickness)\n",
        "                    cv2.putText(image, str(region['class']), (pts[0][0],pts[0][1]), font,  \n",
        "                   2, (0,125,255), 3, cv2.LINE_AA)\n",
        "                    \n",
        "            image_path = os.path.join(self.save_dir ,  '{}_{}_{}.png'.format(self.regions,self.prefix,page_index))            \n",
        "            cv2.imwrite(image_path , image)\n",
        "          \n",
        "    def draw_region_children(self):\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "        fontScale = 2\n",
        "        thickness =3\n",
        "\n",
        "\n",
        "        for page_index in range(len(self.get_page_count())) :\n",
        "            nparr = np.fromstring(self.get_page(page_index), np.uint8)\n",
        "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "            for region_index,region in enumerate(self.get_coords(page_index)) :\n",
        "                try:\n",
        "                    ground = region['boundingBox']['vertices']\n",
        "                    pts = []\n",
        "                    for pt in ground:\n",
        "                        pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "                    #print(pts)\n",
        "                    region_color = (0 ,0,125+ 130*(region_index/ len(self.get_coords(page_index))))\n",
        "                    cv2.polylines(image, [np.array(pts)],True, region_color, self.thickness)\n",
        "                    cv2.putText(image, str(region_index), (pts[0][0],pts[0][1]), font,  \n",
        "                   fontScale, region_color, thickness, cv2.LINE_AA)\n",
        "                    for line_index, line in enumerate(region['children']):\n",
        "                        ground = line['boundingBox']['vertices']\n",
        "                        pts = []\n",
        "                        for pt in ground:\n",
        "                            pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "\n",
        "                        line_color = (125 + 130*(region_index/ len(self.get_coords(page_index))) ,0,0)\n",
        "                        cv2.polylines(image, [np.array(pts)],True, line_color, self.thickness -2)\n",
        "                        cv2.putText(image, str(line_index), (pts[0][0],pts[0][1]), font,  \n",
        "                   fontScale, line_color, thickness, cv2.LINE_AA)\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "                    print(region)\n",
        "                    \n",
        "            image_path = os.path.join(self.save_dir ,  '{}_{}.png'.format(self.prefix,page_index))\n",
        "            cv2.imwrite(image_path , image)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiprWTP0UXKM"
      },
      "source": [
        "def draw_region(image,regions,color,key):\n",
        "    nparr = np.fromstring(image, np.uint8)\n",
        "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    for region in regions :\n",
        "            ground = region[key]['boundingBox']['vertices']\n",
        "            pts = []\n",
        "            for pt in ground:\n",
        "                pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "            cv2.polylines(image, [np.array(pts)],True,color, 3)\n",
        "\n",
        "    return image\n",
        "   \n",
        "\n",
        "def draw_compare(image,regions):\n",
        "    nparr = np.fromstring(image, np.uint8)\n",
        "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    for region in regions :\n",
        "        if region['ground'] != None :\n",
        "            ground = region['ground']['boundingBox']['vertices']\n",
        "            pts = []\n",
        "            for pt in ground:\n",
        "                pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "            cv2.polylines(image, [np.array(pts)],True,(255,0,0), 3)\n",
        "\n",
        "        if region['input'] != None :\n",
        "            inpu_t = region['input']['boundingBox']['vertices']\n",
        "            pts = []\n",
        "            for pt in inpu_t:\n",
        "                pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "            cv2.polylines(image, [np.array(pts)],True,(0,0,255), 3)         \n",
        "    return image\n",
        "    \n",
        "    \n",
        "      \n",
        "def draw_erros(page_row,page_index):\n",
        "    page_path = page_row['path'].split('upload')[1]\n",
        "    image = download_file(download_url,headers,page_path,f_type='image')\n",
        "    not_in_pred = draw_region(image,page_row['only_in_ground'],(255,0,0),'ground')\n",
        "    not_in_ground = draw_region(image,page_row['only_in_pred'],(0,0,255),'input' )\n",
        "    compare       = draw_compare(image,page_row['below_iou_threshold'])\n",
        "    return np.hstack([not_in_pred,compare,not_in_ground])\n",
        "\n",
        "        \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkjuBzgdUXKN"
      },
      "source": [
        "\n",
        "def find_erros(eval_data,input_data,iou_threshold):\n",
        "  \n",
        "    count_erros = []\n",
        "    iou_error   = []\n",
        "    for file_index in range(len(eval_data['rsp']['outputs'])) :\n",
        "        file_name = eval_data['rsp']['input']['inputs'][file_index]['input']['name']\n",
        "        \n",
        "        for page_index in range(len(eval_data['rsp']['outputs'][file_index]['pages'])):\n",
        "           \n",
        "    \n",
        "            print('evaluationg file {} page {}'.format(file_index,page_index))\n",
        "\n",
        "            page = eval_data['rsp']['outputs'][file_index]['pages'][page_index]\n",
        "            page_image = input_data['outputs'][file_index]['page_info'][page_index]\n",
        "\n",
        "            regions = {}\n",
        "            #try :\n",
        "            counts = page['count'] \n",
        "            count_diff  = abs(counts['input'] - counts['ground'])\n",
        "            #print('diff in boxes is {}'.format(count_diff))\n",
        "\n",
        "           # if count_diff >= count_threshold :\n",
        "                    #count_erros.append({'file_index' : file_index ,'page_index': page_index,'page':page,'path':page_image})\n",
        "            avg_iou = pd.DataFrame(page['iou'])['iou'].mean()\n",
        "            print('avrage page iou is {} \\n'.format(pd.DataFrame(page['iou'])['iou'].mean()))\n",
        "            #regions = {'below_iou_threshold':[],'avg_iou': avg_iou, 'count_diff':count_diff,'count_input':counts['input'],'count_ground':counts['ground'], 'file_index' : file_index ,'page_index': page_index, 'page':page['iou'],'path':page_image}\n",
        "            regions = {'below_iou_threshold':[],\\\n",
        "                       'avg_iou': avg_iou,'count_input':counts['input'],\\\n",
        "                       'count_ground':counts['ground'], \\\n",
        "                       'file_name' : file_name,\\\n",
        "                       'file_index' : file_index ,'page_index': page_index,\\\n",
        "                       'page':page['iou'],\\\n",
        "                       'only_in_ground' : [],\\\n",
        "                       'only_in_pred'    : [],\n",
        "                       'path':page_image}\n",
        "\n",
        "\n",
        "            for region_iou in page['iou']:\n",
        "                if region_iou['iou'] <= iou_threshold:\n",
        "                    regions['below_iou_threshold'].append(region_iou)\n",
        "\n",
        "            regions['count_below_iou_threshold']= len(regions['below_iou_threshold'])\n",
        "\n",
        "            for region_iou in page['iou']:\n",
        "                if region_iou['ground'] is None:\n",
        "                    regions['only_in_pred'].append(region_iou)\n",
        "\n",
        "                if region_iou['input'] is None :\n",
        "                    regions['only_in_ground'].append(region_iou)\n",
        "\n",
        "            regions['count_below_iou_threshold']= len(regions['below_iou_threshold'])\n",
        "            #iou_error.append(regions)\n",
        "\n",
        "\n",
        "            #except Exception as e :\n",
        "            #    print(e)\n",
        "            \n",
        "            iou_error.append(regions)\n",
        "            \n",
        "    return pd.DataFrame(iou_error)\n",
        "    \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhCxaxP6UXKP"
      },
      "source": [
        "def word_detector(word_url,headers,pdf_name):\n",
        "    #pdf_name = '/home/srihari/Downloads/uploads/13147_2018_6_15_11910_Judgement_29-Jan-2019_ASM.pdf'\n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": pdf_name,\n",
        "            \"type\": file_format,\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_WD\"\n",
        "    }\n",
        "    res = requests.post(word_url,json=file,headers=headers)\n",
        "    return res.json()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpD2xv9BUXKR"
      },
      "source": [
        "def google_ocr(word_url,headers,pdf_name):\n",
        "    \n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": pdf_name,\n",
        "            \"type\": file_format,\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_OGV\"\n",
        "    }\n",
        "    res = requests.post(word_url,json=file,headers=headers)\n",
        "    return res.json()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krVjjNUhUXKT"
      },
      "source": [
        "def tesseract_ocr(layout_url,headers,layout_detector_output,language=language):\n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": layout_detector_output,\n",
        "            \"type\": \"json\",\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_OTES\"\n",
        "    }\n",
        "    res = requests.post(layout_url,json=file,headers=headers)\n",
        "    return res.json()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kksVlXY_UXKU"
      },
      "source": [
        "def layout_detector(layout_url,headers,word_detector_output):\n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": word_detector_output,\n",
        "            \"type\": \"json\",\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_LD\"\n",
        "    }\n",
        "    res = requests.post(layout_url,json=file,headers=headers)\n",
        "    return res.json()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REj8kDJyUXKV"
      },
      "source": [
        "def block_segmenter(segmenter_url,headers,layout_detector_output):\n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": layout_detector_output,\n",
        "            \"type\": \"json\",\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_BS\"\n",
        "    }\n",
        "    res = requests.post(layout_url,json=file,headers=headers)\n",
        "    return res.json()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhA2cUxHUXKV"
      },
      "source": [
        "def evaluator(evaluator_url,headers,input_json):\n",
        "    file = {\n",
        "                \"input\": { \"inputs\": [\n",
        "                {\n",
        "                  \"input\": {\n",
        "                    \"jobId\": \"string\",\n",
        "                    \"name\": input_json['predicted'],\n",
        "                    \"type\": \"json\"\n",
        "                  },\n",
        "                  \"ground\": {\n",
        "                    \"jobId\": \"string\",\n",
        "                    \"name\": input_json['ground'],\n",
        "                    \"type\": \"json\"\n",
        "                  },\n",
        "                  \"config\": {\n",
        "                    \"strategy\": \"IOU\",\n",
        "                    \"boxLevel\": input_json['level']\n",
        "                  }\n",
        "                } ]}\n",
        "\n",
        "                }\n",
        "    \n",
        "    res = requests.post(evaluator_url,json=file)\n",
        "    \n",
        "    \n",
        "    return res.json()\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf7ShIWeUXKW"
      },
      "source": [
        "def upload_file(pdf_file,headers,url):\n",
        "    #url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "    files = [\n",
        "        ('file',(open(pdf_file,'rb')))] \n",
        "\n",
        "    response = requests.post(url, headers=headers, files=files)\n",
        "    \n",
        "    return response.json()\n",
        "    response.json()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv1XcQKSUXKW"
      },
      "source": [
        "def download_file(download_url,headers,outputfile,f_type='json'):\n",
        "    download_url =download_url+str(outputfile)\n",
        "    res = requests.get(download_url,headers=headers)\n",
        "    if f_type == 'json':\n",
        "        return res.json()\n",
        "    else :\n",
        "        return res.content"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bovbQq95UXKX"
      },
      "source": [
        "def save_json(path,res):\n",
        "    with open(path, \"w\", encoding='utf8') as write_file:\n",
        "        json.dump(res, write_file,ensure_ascii=False )\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JjWXxYZUXKX"
      },
      "source": [
        "def bulk_search(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    print(res.json())\n",
        "    \n",
        "   \n",
        "    while(1):\n",
        "        \n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "       \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']\n",
        "            print(outputfile)\n",
        "            print(job_id)\n",
        "            return outputfile\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        print(progress)\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "        #print(res.json())\n",
        "      \n",
        "   "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZclZb2cUXKX"
      },
      "source": [
        "\n",
        "def execute_module(module,url,input_file,module_code,pdf_dir,overwirte=True , draw=True):\n",
        "    \n",
        "        \n",
        "        \n",
        "        output_path = os.path.join(pdf_dir,'{}.json'.format(module_code))\n",
        "        if os.path.exists(output_path) and not overwirte:\n",
        "            print(' loading *****************{}'.format(module_code ))\n",
        "            with open(output_path,'r') as wd_file :\n",
        "                response = json.load(wd_file)\n",
        "                \n",
        "            wf_res = pdf_dir + '/{}_wf.json'.format(module_code)\n",
        "            with open(wf_res,'r') as wd_file :\n",
        "                json_file = json.load(wd_file) \n",
        "            #json_file = upload_file(output_path,headers,upload_url)['data']\n",
        "        else :\n",
        "            if module_code in ['wd','gv']:\n",
        "                res = upload_file(input_file,headers,upload_url)\n",
        "                print('upload response **********', res)\n",
        "                pdf_name = res['data']\n",
        "                response = module(url,headers,pdf_name)\n",
        "            \n",
        "            else : \n",
        "                response = module(url,headers,input_file)\n",
        "                \n",
        "                if 'eval' in module_code :\n",
        "                    json_file = response['outputFile']\n",
        "                    response = download_file(download_url,headers,json_file)\n",
        "                    save_json(output_path,response)\n",
        "                    return json_file,response\n",
        "                \n",
        "            \n",
        "            print(' response *****************{} {}'.format(module_code ,response ))\n",
        "            job_id = response['jobID']\n",
        "            json_file = bulk_search(job_id,bs_url,headers)\n",
        "            save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   \n",
        "            print('bulk search  response **************',json_file )\n",
        "            response = download_file(download_url,headers,json_file)\n",
        "            save_json(output_path,response)\n",
        "            if draw :\n",
        "                if module_code in ['wd','gv']:\n",
        "                    Draw(response,pdf_dir,regions='lines',prefix=module_code)\n",
        "                else :\n",
        "                     Draw(response,pdf_dir,regions='regions',prefix=module_code)\n",
        "                    \n",
        "        return json_file,response\n",
        "        \n",
        "            \n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czH1ng9gUXKY"
      },
      "source": [
        "def evaluate__and_save_input(pdf_files,output_dir,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
        "    word_responses   = {}\n",
        "    layout_responses = {}\n",
        "    segmenter_responses = []\n",
        "    for pdf in pdf_files:\n",
        "        # try :\n",
        "        pdf_name = pdf.split('/')[-1].split('.')[0]\n",
        "        print(pdf , ' is being processed')\n",
        "        pdf_output_dir = os.path.join(output_dir,pdf_name)\n",
        "        os.system('mkdir -p \"{}\"'.format(pdf_output_dir))\n",
        "\n",
        "\n",
        "        # wd_json,_ = execute_module(word_detector,word_url,input_file=pdf,\\\n",
        "        #                module_code='wd',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "        google_json,google_resposne = execute_module(google_ocr,google_url,input_file=pdf,\\\n",
        "                       module_code='gv',pdf_dir=pdf_output_dir,overwirte=False , draw=False)\n",
        "\n",
        "\n",
        "        # ld_json,_ = execute_module(layout_detector,layout_url,input_file=wd_json,\\\n",
        "        #                module_code='ld',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "        # seg_json,_ = execute_module(block_segmenter,segmenter_url,input_file=ld_json,\\\n",
        "        #                module_code='seg',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # tess_json,_= execute_module(tesseract_ocr,word_url,input_file=seg_json,\\\n",
        "        #             module_code='tess',pdf_dir=pdf_output_dir,overwirte=True , draw=True)\n",
        "\n",
        "\n",
        "        # evaluator_input= {'predicted': tess_json , 'ground':google_json ,'level': evaluation_level}\n",
        "\n",
        "        # eval_json ,evaluator_response = execute_module(evaluator,evaluator_url,\\\n",
        "        #                                                 input_file=evaluator_input,module_code='eval_line', \\\n",
        "        #                                                 pdf_dir= pdf_output_dir)\n",
        "\n",
        "\n",
        "        # error_df = find_erros(evaluator_response,google_resposne,iou_threshold=0.33)\n",
        "        # for page_index,page_row in error_df.iterrows():\n",
        "        #     img = draw_erros(page_row,page_index)\n",
        "        #     cv2.imwrite('{}/eval_{}_{}.png'.format(pdf_output_dir,evaluation_level,page_index),np.array(img).astype('uint8'))\n",
        "\n",
        "\n",
        "          \n",
        "        # except Exception as e : \n",
        "        #     print(e)\n",
        "        #     logging.error('error in file {}  \\n {}'.format(pdf_name,e))\n",
        "\n",
        " \n",
        "    \n",
        "    \n",
        "        return google_resposne,pdf_output_dir\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVhnhgQyUXKY"
      },
      "source": [
        "def main(path,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
        "        pdf_names = glob.glob(path + '/*.pdf')\n",
        "        pdf_names.reverse()\n",
        "        \n",
        "        return evaluate__and_save_input(pdf_names,output_path,headers,word_url,layout_url,download_url,upload_url,bs_url)\n",
        "        "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6ZMNoyfnUXKZ"
      },
      "source": [
        "main(path,headers,word_url,layout_url,download_url,upload_url,bs_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h11rpLNiUXKa"
      },
      "source": [
        "# evaluator_response,google_resposne,pdf_output_dir= main(path,headers,word_url,layout_url,download_url,upload_url,bs_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u09H0j6oMQ7"
      },
      "source": [
        "import json \r\n",
        "\r\n",
        "with open('/content/lrs1_te/gv.json') as f:\r\n",
        "  data = json.load(f)\r\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LKGOeXHIoXuD",
        "outputId": "f3b53e99-a4d6-4235-8c54-08a6ff739bcd"
      },
      "source": [
        "data['tool']"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'OCR-GOOGLE-VISION'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LjXD_jUoi0U",
        "outputId": "b2d9a92c-8528-4b5b-d2c3-f4385cfd3b17"
      },
      "source": [
        "data['outputs'][0].keys()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['status', 'page_info', 'file', 'pages'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY08fYwjt7GB"
      },
      "source": [
        "pages = len(data['outputs'][0]['pages'])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdWP_883pyVb"
      },
      "source": [
        "# lines = len(data['outputs'][0]['pages'][page]['lines'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glGHt2tRq6Br"
      },
      "source": [
        "text = ''\r\n",
        "for page in range(pages):\r\n",
        "  lines = len(data['outputs'][0]['pages'][page]['lines']) \r\n",
        "  for i in range(lines): \r\n",
        "      text+= data['outputs'][0]['pages'][page]['lines'][i]['text']\r\n",
        "      text+='\\n'\r\n",
        "    "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ybHEJXrO6n"
      },
      "source": [
        "with open('/content/ap_lrs1.txt','w') as file:\r\n",
        "  file.write(text)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn8oXhJCyUt6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}