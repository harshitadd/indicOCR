# -*- coding: utf-8 -*-
"""VisionOCR_JSONtoTXT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SagcL9SuOX8nQHWiP1WeeYh0gPLxNtAW
"""

import glob
import uuid
import json
import requests
import copy
import os
import numpy as np
from time import sleep
import pandas as pd
import logging

path = '/home/harshita/AI4BharatTranslation/te_source/eng/'
output_path = '/home/harshita/AI4BharatTranslation/te_source/eng/'
language = 'detect'
file_format = 'PDF'
evaluation_level = 'LINE'
token ='eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyTmFtZSI6ImhhcnNoaXRhZGRAZ21haWwuY29tIiwicGFzc3dvcmQiOiJiJyQyYiQxMiR2L25RbDQ0dWZHdXhoUEJpZElHYm11S2hMcHVWLmhwQWs5cTF6M2UvNkoueXltUEJnUFNkTyciLCJleHAiOjE2MTQ0MDAzNDN9.eNtXbLuufgaU3q69mUSDxSjXmmcs3FO9s31VurJ5TIE'


class Draw:
    
    def __init__(self,input_json,save_dir,regions,prefix='',color= (255,0,0),thickness=5):   
        self.json = input_json
        self.save_dir = save_dir
        self.regions = regions
        self.prefix  = prefix
        self.color  = color
        self.thickness=thickness
        if self.prefix in ['seg','tess']:
            #print('drawing children')
            self.draw_region_children()
        else:
            self.draw_region()
        
    def get_coords(self,page_index):
        return self.json['outputs'][0]['pages'][page_index][self.regions]
    
    def get_page_count(self):
        return(self.json['outputs'][0]['page_info'])
    
    def get_page(self,page_index):
        page_path = self.json['outputs'][0]['page_info'][page_index]
        page_path = page_path.split('upload')[1]#'/'.join(page_path.split('/')[1:])
        print(page_path)    
        return download_file(download_url,headers,page_path,f_type='image')

    def draw_region(self):
        font = cv2.FONT_HERSHEY_SIMPLEX 
        for page_index in range(len(self.get_page_count())) :
            nparr = np.fromstring(self.get_page(page_index), np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            for region in self.get_coords(page_index) :
                    ground = region['boundingBox']['vertices']
                    pts = []
                    for pt in ground:
                        pts.append([int(pt['x']) ,int(pt['y'])])
                    cv2.polylines(image, [np.array(pts)],True, self.color, self.thickness)
                    cv2.putText(image, str(region['class']), (pts[0][0],pts[0][1]), font,  
                   2, (0,125,255), 3, cv2.LINE_AA)
                    
            image_path = os.path.join(self.save_dir ,  '{}_{}_{}.png'.format(self.regions,self.prefix,page_index))            
            cv2.imwrite(image_path , image)
          
    def draw_region_children(self):
        font = cv2.FONT_HERSHEY_SIMPLEX 
        fontScale = 2
        thickness =3


        for page_index in range(len(self.get_page_count())) :
            nparr = np.fromstring(self.get_page(page_index), np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            for region_index,region in enumerate(self.get_coords(page_index)) :
                try:
                    ground = region['boundingBox']['vertices']
                    pts = []
                    for pt in ground:
                        pts.append([int(pt['x']) ,int(pt['y'])])
                    #print(pts)
                    region_color = (0 ,0,125+ 130*(region_index/ len(self.get_coords(page_index))))
                    cv2.polylines(image, [np.array(pts)],True, region_color, self.thickness)
                    cv2.putText(image, str(region_index), (pts[0][0],pts[0][1]), font,  
                   fontScale, region_color, thickness, cv2.LINE_AA)
                    for line_index, line in enumerate(region['children']):
                        ground = line['boundingBox']['vertices']
                        pts = []
                        for pt in ground:
                            pts.append([int(pt['x']) ,int(pt['y'])])

                        line_color = (125 + 130*(region_index/ len(self.get_coords(page_index))) ,0,0)
                        cv2.polylines(image, [np.array(pts)],True, line_color, self.thickness -2)
                        cv2.putText(image, str(line_index), (pts[0][0],pts[0][1]), font,  
                   fontScale, line_color, thickness, cv2.LINE_AA)
                except Exception as e:
                    print(str(e))
                    print(region)
                    
            image_path = os.path.join(self.save_dir ,  '{}_{}.png'.format(self.prefix,page_index))
            cv2.imwrite(image_path , image)

def draw_region(image,regions,color,key):
    nparr = np.fromstring(image, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    for region in regions :
            ground = region[key]['boundingBox']['vertices']
            pts = []
            for pt in ground:
                pts.append([int(pt['x']) ,int(pt['y'])])
            cv2.polylines(image, [np.array(pts)],True,color, 3)

    return image
   

def draw_compare(image,regions):
    nparr = np.fromstring(image, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    for region in regions :
        if region['ground'] != None :
            ground = region['ground']['boundingBox']['vertices']
            pts = []
            for pt in ground:
                pts.append([int(pt['x']) ,int(pt['y'])])
            cv2.polylines(image, [np.array(pts)],True,(255,0,0), 3)

        if region['input'] != None :
            inpu_t = region['input']['boundingBox']['vertices']
            pts = []
            for pt in inpu_t:
                pts.append([int(pt['x']) ,int(pt['y'])])
            cv2.polylines(image, [np.array(pts)],True,(0,0,255), 3)         
    return image
    
    
      
def draw_erros(page_row,page_index):
    page_path = page_row['path'].split('upload')[1]
    image = download_file(download_url,headers,page_path,f_type='image')
    not_in_pred = draw_region(image,page_row['only_in_ground'],(255,0,0),'ground')
    not_in_ground = draw_region(image,page_row['only_in_pred'],(0,0,255),'input' )
    compare       = draw_compare(image,page_row['below_iou_threshold'])
    return np.hstack([not_in_pred,compare,not_in_ground])

def find_erros(eval_data,input_data,iou_threshold):
  
    count_erros = []
    iou_error   = []
    for file_index in range(len(eval_data['rsp']['outputs'])) :
        file_name = eval_data['rsp']['input']['inputs'][file_index]['input']['name']
        
        for page_index in range(len(eval_data['rsp']['outputs'][file_index]['pages'])):
           
    
            print('evaluationg file {} page {}'.format(file_index,page_index))

            page = eval_data['rsp']['outputs'][file_index]['pages'][page_index]
            page_image = input_data['outputs'][file_index]['page_info'][page_index]

            regions = {}
            #try :
            counts = page['count'] 
            count_diff  = abs(counts['input'] - counts['ground'])
            #print('diff in boxes is {}'.format(count_diff))

           # if count_diff >= count_threshold :
                    #count_erros.append({'file_index' : file_index ,'page_index': page_index,'page':page,'path':page_image})
            avg_iou = pd.DataFrame(page['iou'])['iou'].mean()
            print('avrage page iou is {} \n'.format(pd.DataFrame(page['iou'])['iou'].mean()))
            #regions = {'below_iou_threshold':[],'avg_iou': avg_iou, 'count_diff':count_diff,'count_input':counts['input'],'count_ground':counts['ground'], 'file_index' : file_index ,'page_index': page_index, 'page':page['iou'],'path':page_image}
            regions = {'below_iou_threshold':[],\
                       'avg_iou': avg_iou,'count_input':counts['input'],\
                       'count_ground':counts['ground'], \
                       'file_name' : file_name,\
                       'file_index' : file_index ,'page_index': page_index,\
                       'page':page['iou'],\
                       'only_in_ground' : [],\
                       'only_in_pred'    : [],
                       'path':page_image}


            for region_iou in page['iou']:
                if region_iou['iou'] <= iou_threshold:
                    regions['below_iou_threshold'].append(region_iou)

            regions['count_below_iou_threshold']= len(regions['below_iou_threshold'])

            for region_iou in page['iou']:
                if region_iou['ground'] is None:
                    regions['only_in_pred'].append(region_iou)

                if region_iou['input'] is None :
                    regions['only_in_ground'].append(region_iou)

            regions['count_below_iou_threshold']= len(regions['below_iou_threshold'])
            #iou_error.append(regions)


            #except Exception as e :
            #    print(e)
            
            iou_error.append(regions)
            
    return pd.DataFrame(iou_error)

def word_detector(word_url,headers,pdf_name):
    #pdf_name = '/home/srihari/Downloads/uploads/13147_2018_6_15_11910_Judgement_29-Jan-2019_ASM.pdf'
    file = {
       "files": [
        {
            "locale": "en",
            "path": pdf_name,
            "type": file_format,
            "config":{
        "OCR": {
          "option": "HIGH_ACCURACY",
          "language": language
        }
        }}
    ],
    "workflowCode": "WF_A_WD"
    }
    res = requests.post(word_url,json=file,headers=headers)
    return res.json()

def google_ocr(word_url,headers,pdf_name):
    
    file = {
       "files": [
        {
            "locale": "en",
            "path": pdf_name,
            "type": file_format,
            "config":{
        "OCR": {
          "option": "HIGH_ACCURACY",
          "language": language
        }
        }}
    ],
    "workflowCode": "WF_A_OGV"
    }
    res = requests.post(word_url,json=file,headers=headers)
    return res.json()

def tesseract_ocr(layout_url,headers,layout_detector_output,language=language):
    file = {
       "files": [
        {
            "locale": "en",
            "path": layout_detector_output,
            "type": "json",
            "config":{
        "OCR": {
          "option": "HIGH_ACCURACY",
          "language": language
        }
        }}
    ],
    "workflowCode": "WF_A_OTES"
    }
    res = requests.post(layout_url,json=file,headers=headers)
    return res.json()

def layout_detector(layout_url,headers,word_detector_output):
    file = {
       "files": [
        {
            "locale": "en",
            "path": word_detector_output,
            "type": "json",
            "config":{
        "OCR": {
          "option": "HIGH_ACCURACY",
          "language": language
        }
        }}
    ],
    "workflowCode": "WF_A_LD"
    }
    res = requests.post(layout_url,json=file,headers=headers)
    return res.json()

def block_segmenter(segmenter_url,headers,layout_detector_output):
    file = {
       "files": [
        {
            "locale": "en",
            "path": layout_detector_output,
            "type": "json",
            "config":{
        "OCR": {
          "option": "HIGH_ACCURACY",
          "language": language
        }
        }}
    ],
    "workflowCode": "WF_A_BS"
    }
    res = requests.post(layout_url,json=file,headers=headers)
    return res.json()

def evaluator(evaluator_url,headers,input_json):
    file = {
                "input": { "inputs": [
                {
                  "input": {
                    "jobId": "string",
                    "name": input_json['predicted'],
                    "type": "json"
                  },
                  "ground": {
                    "jobId": "string",
                    "name": input_json['ground'],
                    "type": "json"
                  },
                  "config": {
                    "strategy": "IOU",
                    "boxLevel": input_json['level']
                  }
                } ]}

                }
    
    res = requests.post(evaluator_url,json=file)
    
    
    return res.json()

def upload_file(pdf_file,headers,url):
    #url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'
    files = [
        ('file',(open(pdf_file,'rb')))] 

    response = requests.post(url, headers=headers, files=files)
    
    return response.json()
    response.json()

# upload_file("/Users/eaxxkra/Downloads/tn_budget_text/GA_2020_ENG_en.txt",headers,upload_url)

def download_file(download_url,headers,outputfile,f_type='json'):
    download_url =download_url+str(outputfile)
    res = requests.get(download_url,headers=headers)
    if f_type == 'json':
        return res.json()
    else :
        return res.content

def save_json(path,res):
    with open(path, "w", encoding='utf8') as write_file:
        json.dump(res, write_file,ensure_ascii=False )

def bulk_search(job_id,bs_url,headers):
    bs_request = {
    "jobIDs": [job_id],
    "taskDetails":"true"
    }
    print(job_id)
    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)
    print(res.json())
    prev_progress = ""
   
    while(1):
        
        progress = res.json()['jobs'][0]['status']
       
        if progress in ['COMPLETED','FAILED']:
            print(progress)
            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']
            print(outputfile)
            print(job_id)
            return outputfile
            break
        sleep(0.5)
        if progress != prev_progress:
            print(progress)
            prev_progress = progress
        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)
        #print(res.json())

def bulk_search_all(job_id,bs_url,headers):
    bs_request = {
    "jobIDs": [job_id],
    "taskDetails":"true"
    }
    print(job_id)
    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)
    print(res.json())
    
   
    while(1):
        
        progress = res.json()['jobs'][0]['status']
       
        if progress in ['COMPLETED','FAILED']:
            print(progress)
            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']
            print(outputfile)
            print(job_id)
            return outputfile
            break
        sleep(0.5)
        print(progress)
        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)

def execute_module(module,url,input_file,module_code,pdf_dir,overwirte=True , draw=True):
    
        
        
        output_path = os.path.join(pdf_dir,'{}.json'.format(module_code))
        if os.path.exists(output_path) and not overwirte:
            print(' loading *****************{}'.format(module_code ))
            with open(output_path,'r') as wd_file :
                response = json.load(wd_file)
                
            wf_res = pdf_dir + '/{}_wf.json'.format(module_code)
            with open(wf_res,'r') as wd_file :
                json_file = json.load(wd_file) 
            #json_file = upload_file(output_path,headers,upload_url)['data']
            job_to_add = (pdf_dir)

        else :
            if module_code in ['wd','gv']:
                res = upload_file(input_file,headers,upload_url)
                print('upload response **********', res)
                pdf_name = res['data']
                response = module(url,headers,pdf_name)
            
            else : 
                response = module(url,headers,input_file)
                
                if 'eval' in module_code :
                    json_file = response['outputFile']
                    response = download_file(download_url,headers,json_file)
                    save_json(output_path,response)
                    return json_file,response
                
            
            print(' response *****************{} {}'.format(module_code ,response ))
            job_id = response['jobID']
            
            job_to_add = (job_id,bs_url,headers,pdf_dir,module_code,output_path)
#             json_file = bulk_search(job_id,bs_url,headers)
#             save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   
#             print('bulk search  response **************',json_file )
#             response = download_file(download_url,headers,json_file)
#             save_json(output_path,response)
            
#             if draw :
#                 if module_code in ['wd','gv']:
#                     Draw(response,pdf_dir,regions='lines',prefix=module_code)
#                 else :
#                      Draw(response,pdf_dir,regions='regions',prefix=module_code)
        return job_to_add
#         return json_file,response



def evaluate__and_save_input(pdf_files,output_dir,headers,word_url,layout_url,download_url,upload_url,bs_url):
    word_responses   = {}
    layout_responses = {}
    segmenter_responses = []
    jobs_to_track = []
    for pdf in pdf_files:
        try :
            pdf_name = pdf.split('/')[-1].split('.')[0]
            print(pdf , ' is being processed')
            pdf_output_dir = os.path.join(output_dir,pdf_name)
            os.system('mkdir -p "{}"'.format(pdf_output_dir))


#             wd_json,_ = execute_module(word_detector,word_url,input_file=pdf,\
#                            module_code='wd',pdf_dir=pdf_output_dir,overwirte=False , draw=True)


#             google_json,google_resposne = 
            job_to_add = execute_module(google_ocr,google_url,input_file=pdf,\
                           module_code='gv',pdf_dir=pdf_output_dir,overwirte=False , draw=False)
            
            jobs_to_track.append(job_to_add)
#             ld_json,_ = execute_module(layout_detector,layout_url,input_file=wd_json,\
#                            module_code='ld',pdf_dir=pdf_output_dir,overwirte=False , draw=True)


#             seg_json,_ = execute_module(block_segmenter,segmenter_url,input_file=ld_json,\
#                            module_code='seg',pdf_dir=pdf_output_dir,overwirte=False , draw=True)





#             tess_json,_= execute_module(tesseract_ocr,word_url,input_file=seg_json,\
#                         module_code='tess',pdf_dir=pdf_output_dir,overwirte=True , draw=True)


#             evaluator_input= {'predicted': tess_json , 'ground':google_json ,'level': evaluation_level}

#             eval_json ,evaluator_response = execute_module(evaluator,evaluator_url,\
#                                                            input_file=evaluator_input,module_code='eval_line', \
#                                                            pdf_dir= pdf_output_dir)


#             error_df = find_erros(evaluator_response,google_resposne,iou_threshold=0.33)
#             #error_df
#             for page_index,page_row in error_df.iterrows():
#                 img = draw_erros(page_row,page_index)
#                 cv2.imwrite('{}/eval_{}_{}.png'.format(pdf_output_dir,evaluation_level,page_index),np.array(img).astype('uint8'))
        

          
        except Exception as e : 
            print(e)
            logging.error('error in file {}  \n {}'.format(pdf_name,e))

 
    for job in jobs_to_track:
        print("----------")
        print(job)
        job_id,bs_url,headers,pdf_dir,module_code,output_path = job
        json_file = bulk_search(job_id,bs_url,headers)
        save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   
        print('bulk search  response **************',json_file )
        response = download_file(download_url,headers,json_file)
        save_json(output_path,response)

    print("----------------")
    print(jobs_to_track)
    print("----------------")
    return jobs_to_track

def main(path,headers,word_url,layout_url,download_url,upload_url,bs_url):
        pdf_names = glob.glob(path + '/*.pdf')
        pdf_names.reverse()
        
        return evaluate__and_save_input(pdf_names,output_path,headers,word_url,layout_url,download_url,upload_url,bs_url)

if __name__ == '__main__':
    main(path,headers,word_url,layout_url,download_url,upload_url,bs_url)

    folders = os.listdir(path)
    for folder in folders: 
        if 'te' in folder:
            fn = path + folder + '/gv.json'
            if '.pdf' not in fn:
                with open(fn,'r') as f:
                    data = json.load(f)
                    pages = data['outputs'][0]['pages']
                    all_texts = []
                    for page in pages:
                        page_text = []
                        lines = page['lines']
                        for line in lines:
                            page_text.append(line['text'])
                        all_texts.append(page_text)
                    all_lines = [line for al in all_texts for line in al]
                    all_text = "\n".join(all_lines)
                f.close()
            outfn = path + folder +'.txt'
            with open(outfn,'w') as f:
                f.write(all_text)
            f.close()

