{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompletePipelineWithCleanup.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "PIB py3.7",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/AI4BharatTranslation/blob/main/CompletePipelineWithCleanup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWlI0TLP6_F"
      },
      "source": [
        "### Parse PDFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMdaBYIoP6_H"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "from time import sleep\n",
        "import glob\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT1j3nuiP6_I"
      },
      "source": [
        "#path = '/home/srihari/Desktop/data/data'\n",
        "word_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "google_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "layout_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "segmenter_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "bs_url =\"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/jobs/search/bulk\"\n",
        "\n",
        "evaluator_url  = \"https://auth.anuvaad.org/anuvaad-etl/document-processor/evaluator/v0/process\"\n",
        "\n",
        "#evaluator_url = 'http://0.0.0.0:5001/anuvaad-etl/document-processor/evaluator/v0/process'\n",
        "\n",
        "download_url =\"https://auth.anuvaad.org/download/\"\n",
        "upload_url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "token = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyTmFtZSI6Im9uZXJhZ2hhdmFuQGdtYWlsLmNvbSIsInBhc3N3b3JkIjoiYickMmIkMTIkWktjei5LZVdBcFkwSkI2NGlmZ1ppZWFSLk85RkN2blB1MGlPeE41Y1F4WTVLOHZaL0UwWmknIiwiZXhwIjoxNjE0Nzg0OTg4fQ.KmRYUkZr9IeDQydevwFoTboHUH_Q7GMBqaWkXZJiBe0'\n",
        "\n",
        "\n",
        "headers = {\n",
        "    'auth-token' :token }\n",
        "language = 'detect'\n",
        "file_format = 'PDF'\n",
        "\n",
        "evaluation_level = 'LINE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npfAt8fpQZCA",
        "outputId": "2b5cc239-c42b-409d-9b10-32e627c128f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOEiDE9-P6_I"
      },
      "source": [
        "parent_folder = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/\"\n",
        "source_pdfs = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/\"\n",
        "parsed_pdf_dir = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/parsed/\"\n",
        "text_dir = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/text/\"\n",
        "sentences_dir = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/\"\n",
        "aligned_sentences_path = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/aligned/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVq8hmgtP6_J"
      },
      "source": [
        "def google_ocr(word_url,headers,pdf_name):\n",
        "    \n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": pdf_name,\n",
        "            \"type\": file_format,\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_OGV\"\n",
        "    }\n",
        "    res = requests.post(word_url,json=file,headers=headers)\n",
        "    return res.json()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "082r5MXVP6_J"
      },
      "source": [
        "def upload_file(pdf_file,headers,url):\n",
        "    #url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "    files = [\n",
        "        ('file',(open(pdf_file,'rb')))] \n",
        "\n",
        "    response = requests.post(url, headers=headers, files=files)\n",
        "    \n",
        "    return response.json()\n",
        "    response.json()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myhs2ktDP6_K"
      },
      "source": [
        "def download_file(download_url,headers,outputfile,f_type='json'):\n",
        "    download_url =download_url+str(outputfile)\n",
        "    res = requests.get(download_url,headers=headers)\n",
        "    if f_type == 'json':\n",
        "        return res.json()\n",
        "    else :\n",
        "        return res.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc75CcCoP6_K"
      },
      "source": [
        "def save_json(path,res):\n",
        "    with open(path, \"w\", encoding='utf8') as write_file:\n",
        "        json.dump(res, write_file,ensure_ascii=False )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyiyvcVjP6_K"
      },
      "source": [
        "def bulk_search(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    print(res.json())\n",
        "    prev_progress = \"\"\n",
        "   \n",
        "    while(1):\n",
        "        \n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "       \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']\n",
        "            print(outputfile)\n",
        "            print(job_id)\n",
        "            return outputfile\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        if progress != prev_progress:\n",
        "            print(progress)\n",
        "            prev_progress = progress\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "        #print(res.json())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymcVysLkP6_L"
      },
      "source": [
        "def bulk_search_all(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    print(res.json())\n",
        "    \n",
        "   \n",
        "    while(1):\n",
        "        \n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "       \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']\n",
        "            print(outputfile)\n",
        "            print(job_id)\n",
        "            return outputfile\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        print(progress)\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzlipU-ZP6_L"
      },
      "source": [
        "def execute_module(module,url,input_file,module_code,pdf_dir,overwirte=True , draw=True):\n",
        "    \n",
        "        \n",
        "        output_path = os.path.join(pdf_dir,'{}.json'.format(module_code))\n",
        "        if os.path.exists(output_path) and not overwirte:\n",
        "            print(' loading *****************{}'.format(module_code ))\n",
        "            with open(output_path,'r') as wd_file :\n",
        "                response = json.load(wd_file)\n",
        "                \n",
        "            wf_res = pdf_dir + '/{}_wf.json'.format(module_code)\n",
        "            with open(wf_res,'r') as wd_file :\n",
        "                json_file = json.load(wd_file) \n",
        "            #json_file = upload_file(output_path,headers,upload_url)['data']\n",
        "            job_to_add = (pdf_dir)\n",
        "\n",
        "        else :\n",
        "            if module_code in ['wd','gv']:\n",
        "                res = upload_file(input_file,headers,upload_url)\n",
        "                print('upload response **********', res)\n",
        "                pdf_name = res['data']\n",
        "                response = module(url,headers,pdf_name)\n",
        "            \n",
        "            else : \n",
        "                response = module(url,headers,input_file)\n",
        "                \n",
        "                if 'eval' in module_code :\n",
        "                    json_file = response['outputFile']\n",
        "                    response = download_file(download_url,headers,json_file)\n",
        "                    save_json(output_path,response)\n",
        "                    return json_file,response\n",
        "                \n",
        "            \n",
        "            print(' response *****************{} {}'.format(module_code ,response ))\n",
        "            job_id = response['jobID']\n",
        "            \n",
        "            job_to_add = (job_id,bs_url,headers,pdf_dir,module_code,output_path)\n",
        "#             json_file = bulk_search(job_id,bs_url,headers)\n",
        "#             save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   \n",
        "#             print('bulk search  response **************',json_file )\n",
        "#             response = download_file(download_url,headers,json_file)\n",
        "#             save_json(output_path,response)\n",
        "            \n",
        "#             if draw :\n",
        "#                 if module_code in ['wd','gv']:\n",
        "#                     Draw(response,pdf_dir,regions='lines',prefix=module_code)\n",
        "#                 else :\n",
        "#                      Draw(response,pdf_dir,regions='regions',prefix=module_code)\n",
        "        return job_to_add\n",
        "#         return json_file,response\n",
        "        \n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJU1VjifP6_M"
      },
      "source": [
        "def evaluate__and_save_input(pdf_files,output_dir,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
        "    word_responses   = {}\n",
        "    layout_responses = {}\n",
        "    segmenter_responses = []\n",
        "    jobs_to_track = []\n",
        "    for pdf in pdf_files:\n",
        "        try :\n",
        "            pdf_name = pdf.split('/')[-1].split('.')[0]\n",
        "            parent_folder_name = pdf.split('/')[-2]\n",
        "            print(pdf , ' is being processed')\n",
        "            pdf_output_dir = os.path.join(output_dir,parent_folder_name,pdf_name)\n",
        "            os.system('mkdir -p \"{}\"'.format(pdf_output_dir))\n",
        "\n",
        "\n",
        "#             wd_json,_ = execute_module(word_detector,word_url,input_file=pdf,\\\n",
        "#                            module_code='wd',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "#             google_json,google_resposne = \n",
        "            job_to_add = execute_module(google_ocr,google_url,input_file=pdf,\\\n",
        "                           module_code='gv',pdf_dir=pdf_output_dir,overwirte=False , draw=False)\n",
        "            \n",
        "            jobs_to_track.append(job_to_add)\n",
        "#             ld_json,_ = execute_module(layout_detector,layout_url,input_file=wd_json,\\\n",
        "#                            module_code='ld',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "#             seg_json,_ = execute_module(block_segmenter,segmenter_url,input_file=ld_json,\\\n",
        "#                            module_code='seg',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#             tess_json,_= execute_module(tesseract_ocr,word_url,input_file=seg_json,\\\n",
        "#                         module_code='tess',pdf_dir=pdf_output_dir,overwirte=True , draw=True)\n",
        "\n",
        "\n",
        "#             evaluator_input= {'predicted': tess_json , 'ground':google_json ,'level': evaluation_level}\n",
        "\n",
        "#             eval_json ,evaluator_response = execute_module(evaluator,evaluator_url,\\\n",
        "#                                                            input_file=evaluator_input,module_code='eval_line', \\\n",
        "#                                                            pdf_dir= pdf_output_dir)\n",
        "\n",
        "\n",
        "#             error_df = find_erros(evaluator_response,google_resposne,iou_threshold=0.33)\n",
        "#             #error_df\n",
        "#             for page_index,page_row in error_df.iterrows():\n",
        "#                 img = draw_erros(page_row,page_index)\n",
        "#                 cv2.imwrite('{}/eval_{}_{}.png'.format(pdf_output_dir,evaluation_level,page_index),np.array(img).astype('uint8'))\n",
        "        \n",
        "\n",
        "          \n",
        "        except Exception as e : \n",
        "            print(e)\n",
        "            logging.error('error in file {}  \\n {}'.format(pdf_name,e))\n",
        "\n",
        " \n",
        "    for job in jobs_to_track:\n",
        "        print(\"----------\")\n",
        "        print(job)\n",
        "        job_id,bs_url,headers,pdf_dir,module_code,output_path = job\n",
        "        json_file = bulk_search(job_id,bs_url,headers)\n",
        "        save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   \n",
        "        print('bulk search  response **************',json_file )\n",
        "        response = download_file(download_url,headers,json_file)\n",
        "        save_json(output_path,response)\n",
        "\n",
        "    print(\"----------------\")\n",
        "    print(jobs_to_track)\n",
        "    print(\"----------------\")\n",
        "    return jobs_to_track"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKvYZUYVP6_N"
      },
      "source": [
        "def main(path,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
        "        pdf_names = glob.glob(path + '/*/*.pdf')\n",
        "#         pds_names_of_intrest = [p for p in pdf_names if any([i in p for i in ['GA_Speech_2018_.',\"Governor's Address 2001-02-Revised-.\",\"Governor's Address 2005-06- .\",\"Budget_speech2018_19.\"]])]\n",
        "        pdf_names.reverse()\n",
        "#         print(pdf_names)\n",
        "        \n",
        "        return evaluate__and_save_input(pdf_names,parsed_pdf_dir,headers,word_url,layout_url,download_url,upload_url,bs_url)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0MBc3ojP6_N"
      },
      "source": [
        "# bulk_search(\"A_OGV-BVBtL-1614524897583\",bs_url,header)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04ptF-t-P6_O",
        "scrolled": true
      },
      "source": [
        "# path = \"/Users/eaxxkra/Downloads/tn_budget_formated/\"\n",
        "main(source_pdfs,headers,word_url,layout_url,download_url,upload_url,bs_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO4uBcl0P6_P"
      },
      "source": [
        "#### Cleanup failed parses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EGZUQilP6_P"
      },
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "present = defaultdict(lambda :0)\n",
        "for a in glob.glob(parsed_pdf_dir+\"/*/*/*\"):\n",
        "    b = a.split(\"/\")[-3]\n",
        "    present[b] = present[b]+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WVNpyS1P6_Q",
        "outputId": "d171f65a-3d37-41cf-f9a4-400d25eb26df"
      },
      "source": [
        "for key,value in present.items():\n",
        "#     print(key,value)\n",
        "    if value != 4:\n",
        "        print(key)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mrqlrO5P6_Q"
      },
      "source": [
        "folders = []\n",
        "for a in glob.glob(parsed_pdf_dir+\"/*\"):\n",
        "    folders.append(a.split(\"/\")[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3v-wkNRP6_Q",
        "outputId": "afc2a2d9-9397-41ba-a5f8-80140ef28ef1"
      },
      "source": [
        "set(folders).difference(present.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayzs87mSP6_R"
      },
      "source": [
        "# pdf_names = glob.glob(source_pdfs + '/*/*.pdf')\n",
        "# pds_names_of_intrest = [p for p in pdf_names if any([i in p for i in ['GA_Speech_2018_.',\"Governor's Address 2001-02-Revised-.\",\"Governor's Address 2005-06- .\",\"Budget_speech2018_19.\"]])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM-ncQunP6_R"
      },
      "source": [
        "# pds_names_of_intrest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLF9itpAP6_R"
      },
      "source": [
        "### Json to text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vd57sWCP6_R"
      },
      "source": [
        "def extract_text(parent,folder_name):\n",
        "    with open(parsed_pdf_dir + f\"{parent}/{folder_name}/gv.json\",'r') as f:\n",
        "        data = json.load(f)\n",
        "    pages = data['outputs'][0]['pages']\n",
        "    all_texts = []\n",
        "    for page in pages:\n",
        "        page_text = []\n",
        "        lines = page['lines']\n",
        "        for line in lines:\n",
        "            page_text.append(line['text'])\n",
        "        all_texts.append(page_text)\n",
        "    all_lines = [line for al in all_texts for line in al]\n",
        "    all_text = \" \".join(all_lines)\n",
        "    lang = folder_name[-2:]\n",
        "    os.system('mkdir -p \"{}\"'.format(text_dir + f\"{parent}\"))\n",
        "    with open(text_dir + f\"{parent}/{folder_name}.txt\",'w',encoding='utf-16') as f:\n",
        "        f.writelines(all_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DAiZk6XP6_R",
        "outputId": "86e5bf02-48b1-41cd-9d55-0625b2efa35f"
      },
      "source": [
        "import glob\n",
        "import shutil\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "failed_parses = []\n",
        "for name in glob.glob(parsed_pdf_dir + '/*/*'):\n",
        "    folder_name = name.replace(parsed_pdf_dir,\"\")\n",
        "    parent ,  folder_name = folder_name.split(\"/\")\n",
        "    try:\n",
        "        extract_text(parent,folder_name)\n",
        "    except:\n",
        "        print(folder_name)\n",
        "        failed_parses.append(folder_name)\n",
        "        \n",
        "for folder_name in failed_parses:\n",
        "    dirpath = Path(text_dir + f\"{folder_name}\")\n",
        "    if dirpath.exists():\n",
        "        shutil.rmtree(dirpath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "en_21_ta\n",
            "ta_15_ta\n",
            "ta_11_ta\n",
            "ta_9_ta\n",
            "ta_8_ta\n",
            "ta_6_ta\n",
            "en_6_ta\n",
            "ta_5_ta\n",
            "en_5_ta\n",
            "ta_1_ta\n",
            "ta_50_ta\n",
            "ta_49_ta\n",
            "ta_46_ta\n",
            "en_41_ta\n",
            "ta_40_ta\n",
            "en_40_ta\n",
            "ta_39_ta\n",
            "en_39_ta\n",
            "ta_35_ta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tmVrBBkP6_S"
      },
      "source": [
        "#### Cleanup failed pdf to text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3ighpi5P6_S",
        "outputId": "bdb72871-da91-421b-b060-b9ce50fe597b"
      },
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "present = defaultdict(lambda :0)\n",
        "for a in glob.glob(text_dir+\"/*/*\"):\n",
        "    b = a.split(\"/\")[-2]\n",
        "    present[b] = present[b]+1\n",
        "\n",
        "to_cleanup = []\n",
        "for key,value in present.items():\n",
        "#     print(key,value)\n",
        "    if value != 2:\n",
        "        to_cleanup.append(key)\n",
        "        print(key)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21\n",
            "15\n",
            "11\n",
            "9\n",
            "8\n",
            "1\n",
            "51\n",
            "50\n",
            "49\n",
            "46\n",
            "41\n",
            "35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsURp_l8P6_S"
      },
      "source": [
        "for a in glob.glob(text_dir+\"/*\"):\n",
        "    if any(i in a for i in to_cleanup):\n",
        "        shutil.rmtree(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9On4JfwmP6_S"
      },
      "source": [
        "### Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N00gfch2P6_S"
      },
      "source": [
        "def upload_file(file):\n",
        "    url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "    files = [\n",
        "        ('file',(open(file,'rb')))] \n",
        "\n",
        "    response = requests.post(url, headers=headers, files=files)\n",
        "    print(response.json())\n",
        "    return response.json()['data']\n",
        "\n",
        "def tokenize_text(file_path,lang):\n",
        "    body = {\"files\": [\n",
        "            {\n",
        "              \"locale\": lang,\n",
        "              \"path\": file_path,\n",
        "              \"type\": \"txt\",\n",
        "              \"text\": [\"TEST\"]\n",
        "            }]}\n",
        "    url = \"https://auth.anuvaad.org/anuvaad-etl/tokeniser/v0/tokenisation\"\n",
        "    response = requests.post(url, headers=headers,json=body).json()\n",
        "    print(response)\n",
        "    if \"output\" in response:\n",
        "        return response['output'][0]['outputFile']\n",
        "    return None\n",
        "\n",
        "def tokenize_texts(file_path_lang_tup_list):\n",
        "    files = []\n",
        "    for file,lang in file_path_lang_tup_list:\n",
        "        files.append({\n",
        "              \"locale\": lang,\n",
        "              \"path\": file,\n",
        "              \"type\": \"txt\",\n",
        "              \"text\": [\"TEST\"]\n",
        "            })\n",
        "    body = {\"files\": files}\n",
        "    url = \"https://auth.anuvaad.org/anuvaad-etl/tokeniser/v0/tokenisation\"\n",
        "    response = requests.post(url, headers=headers,json=body)\n",
        "    print(response.json())\n",
        "    a = [output['outputFile'] for output in response.json()['output']]\n",
        "    print(a)\n",
        "    return a\n",
        "\n",
        "def download_file(outputfile):\n",
        "\n",
        "    download_url =\"https://auth.anuvaad.org/download/\"+str(outputfile)\n",
        "    res = requests.get(download_url,headers=headers)\n",
        "    return res.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkb75MYlP6_S"
      },
      "source": [
        "# input_path = \"/Users/eaxxkra/Downloads/tn_budget_text/*/*.txt\"\n",
        "# output_path = \"/Users/eaxxkra/Downloads/tn_budget_sentences/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GQwmvQyP6_S",
        "scrolled": true,
        "outputId": "a9cba4ac-408c-4e14-a442-30c8d2734c91"
      },
      "source": [
        "failed_files = []\n",
        "for file in glob.glob(text_dir+ \"*/*.txt\"):  \n",
        "    lang = file.split(\".txt\")[0].split(\"_\")[-1]\n",
        "    file_path = upload_file(file)\n",
        "    print(file_path)\n",
        "    tokenized_file_id = tokenize_text(file_path,lang)\n",
        "    if tokenized_file_id:\n",
        "        content = download_file(tokenized_file_id)\n",
        "        os.system('mkdir -p \"{}\"'.format(sentences_dir+ file.split(\"/\")[-2]))\n",
        "        write_path = sentences_dir+ file.split(\"/\")[-2] + \"/\" + file.split(\"/\")[-1]\n",
        "        with open(write_path,'w',encoding=\"utf-16\") as f:\n",
        "            f.writelines(content.decode('utf-16'))\n",
        "    else:\n",
        "        failed_files.append((file_path,lang))\n",
        "failed_files_again = []\n",
        "for file_path,lang in failed_files:\n",
        "    tokenized_file_id = tokenize_text(file_path,lang)\n",
        "    if tokenized_file_id:\n",
        "        content = download_file(tokenized_file_id)\n",
        "        os.system('mkdir -p \"{}\"'.format(sentences_dir+ file.split(\"/\")[-2]))\n",
        "        write_path = sentences_dir+ file.split(\"/\")[-2] + \"/\" + file.split(\"/\")[-1]\n",
        "        with open(write_path,'w',encoding=\"utf-16\") as f:\n",
        "            f.writelines(content.decode('utf-16'))\n",
        "    else:\n",
        "        failed_files_again.append((file_path,lang))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data': 'c6fc285d-62f9-41fe-b04a-59bc81f72e73.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "c6fc285d-62f9-41fe-b04a-59bc81f72e73.txt\n",
            "{'output': [{'inputFile': 'c6fc285d-62f9-41fe-b04a-59bc81f72e73.txt', 'outputFile': '0-1614778494340957.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS'}\n",
            "{'data': '4b66d2e3-0dc1-4ca5-a5ba-9588280321fa.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "4b66d2e3-0dc1-4ca5-a5ba-9588280321fa.txt\n",
            "{'jobID': 'A_TK-ZUceB-1614772577879', 'output': [{'inputFile': '4b66d2e3-0dc1-4ca5-a5ba-9588280321fa.txt', 'outputFile': '0-16147785000991304.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 0, 'taskEndTime': 1614772645359, 'taskID': 'TOK-1614772577962', 'taskStarttime': 1614772577962, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_TOK'}\n",
            "{'data': '838860c6-68d8-42dc-9b8c-91da1a67bbcc.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "838860c6-68d8-42dc-9b8c-91da1a67bbcc.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '52a1380d-a3e9-4fee-b622-b35a0d66883f.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "52a1380d-a3e9-4fee-b622-b35a0d66883f.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '572ca46a-d1d7-4496-a445-ea17a1c279fe.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "572ca46a-d1d7-4496-a445-ea17a1c279fe.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '8e6c85df-1302-42a7-a8ed-4af385a9da1b.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "8e6c85df-1302-42a7-a8ed-4af385a9da1b.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '4e086a96-30cf-4735-8acb-53f601d4e983.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "4e086a96-30cf-4735-8acb-53f601d4e983.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '09e6223d-49bb-4096-9e6f-3149939f8827.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "09e6223d-49bb-4096-9e6f-3149939f8827.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'ffc1ca0c-a4d9-400a-8a6b-07f0f4a758b3.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "ffc1ca0c-a4d9-400a-8a6b-07f0f4a758b3.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '8bc48182-a223-4316-9972-73f5f55a1e16.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "8bc48182-a223-4316-9972-73f5f55a1e16.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '1ea46fc3-d57f-4839-9b2f-05e864ae1b26.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "1ea46fc3-d57f-4839-9b2f-05e864ae1b26.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '4e880c6a-c55e-495d-aa50-f16a5cb68927.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "4e880c6a-c55e-495d-aa50-f16a5cb68927.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '8128b2d4-0db6-4262-99f3-1651814460e7.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "8128b2d4-0db6-4262-99f3-1651814460e7.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'a5c2f8b9-3ca0-4518-a5a7-605c585e69fc.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "a5c2f8b9-3ca0-4518-a5a7-605c585e69fc.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'b1b7df1f-e0dc-4103-9809-f6403eb4ec5d.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "b1b7df1f-e0dc-4103-9809-f6403eb4ec5d.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '7db5c27d-9eb1-4045-bee5-ed9aad76bc0c.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "7db5c27d-9eb1-4045-bee5-ed9aad76bc0c.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'dd0c0525-8467-4788-9807-ddbcc2a60c7f.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "dd0c0525-8467-4788-9807-ddbcc2a60c7f.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '872282a0-95f0-4eaa-9f70-fa5675a84df7.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "872282a0-95f0-4eaa-9f70-fa5675a84df7.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'f780db4b-d75b-43b4-b66e-359e4e74f6be.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "f780db4b-d75b-43b4-b66e-359e4e74f6be.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '65458703-72e6-4b6b-a287-15a71c6dc7cd.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "65458703-72e6-4b6b-a287-15a71c6dc7cd.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'c28acb1e-6b59-4e37-b2b7-d963ab7885f4.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "c28acb1e-6b59-4e37-b2b7-d963ab7885f4.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '72fa5837-8302-4b5d-9ca3-3756c0b36dda.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "72fa5837-8302-4b5d-9ca3-3756c0b36dda.txt\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': '72fa5837-8302-4b5d-9ca3-3756c0b36dda.txt', 'outputFile': '0-161477971867054.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'data': '75e493af-8b85-4eb2-88b0-dad67b57eb97.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "75e493af-8b85-4eb2-88b0-dad67b57eb97.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '8f1eef67-b20e-4111-9a5d-f1bd0ca1b3e2.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "8f1eef67-b20e-4111-9a5d-f1bd0ca1b3e2.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'b40fb745-6f26-49e6-b51b-bfb0ca551f6b.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "b40fb745-6f26-49e6-b51b-bfb0ca551f6b.txt\n",
            "{'output': [{'inputFile': 'b40fb745-6f26-49e6-b51b-bfb0ca551f6b.txt', 'outputFile': '0-16147799025599632.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS'}\n",
            "{'data': 'f1e6e972-8b71-4159-9903-9ffb88f49de5.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "f1e6e972-8b71-4159-9903-9ffb88f49de5.txt\n",
            "{'jobID': 'A_TK-ZUceB-1614772577879', 'output': [{'inputFile': 'f1e6e972-8b71-4159-9903-9ffb88f49de5.txt', 'outputFile': '0-1614779910952806.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 0, 'taskEndTime': 1614772645359, 'taskID': 'TOK-1614772577962', 'taskStarttime': 1614772577962, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_TOK'}\n",
            "{'data': '8543a3ea-8070-4399-b50d-c42a1181e83d.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "8543a3ea-8070-4399-b50d-c42a1181e83d.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '46dbb677-99ce-4059-afa6-df3efff61487.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "46dbb677-99ce-4059-afa6-df3efff61487.txt\n",
            "{'output': [{'inputFile': '46dbb677-99ce-4059-afa6-df3efff61487.txt', 'outputFile': '0-16147799841996083.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS'}\n",
            "{'data': 'cb4b5b68-8fb9-4c62-bf8f-d9cdb9a0f0a8.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "cb4b5b68-8fb9-4c62-bf8f-d9cdb9a0f0a8.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'e36e793c-eac3-4c1e-865c-5e53e0266c2b.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "e36e793c-eac3-4c1e-865c-5e53e0266c2b.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '8b53ff68-f380-444d-a9be-108d8fdace32.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "8b53ff68-f380-444d-a9be-108d8fdace32.txt\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': '8b53ff68-f380-444d-a9be-108d8fdace32.txt', 'outputFile': '0-16147801614172814.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'data': '98fd0f96-3e38-4e4f-b649-01cd789876cf.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "98fd0f96-3e38-4e4f-b649-01cd789876cf.txt\n",
            "{'jobID': 'A_TK-ZUceB-1614772577879', 'output': [{'inputFile': '98fd0f96-3e38-4e4f-b649-01cd789876cf.txt', 'outputFile': '0-1614780175069634.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 0, 'taskEndTime': 1614772645359, 'taskID': 'TOK-1614772577962', 'taskStarttime': 1614772577962, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_TOK'}\n",
            "{'data': '85c8e042-de21-4aa3-a47f-51da69ebcacc.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "85c8e042-de21-4aa3-a47f-51da69ebcacc.txt\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': '85c8e042-de21-4aa3-a47f-51da69ebcacc.txt', 'outputFile': '0-16147801954571354.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'data': '3cbb58d9-abb0-4143-911e-ed71c9643900.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "3cbb58d9-abb0-4143-911e-ed71c9643900.txt\n",
            "{'output': [{'inputFile': '3cbb58d9-abb0-4143-911e-ed71c9643900.txt', 'outputFile': '0-161478020413317.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS'}\n",
            "{'data': 'b5c9cacc-b6f9-420e-a14a-cf960f1de801.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "b5c9cacc-b6f9-420e-a14a-cf960f1de801.txt\n",
            "{'jobID': 'A_TK-ZUceB-1614772577879', 'output': [{'inputFile': 'b5c9cacc-b6f9-420e-a14a-cf960f1de801.txt', 'outputFile': '0-161478021346132.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 0, 'taskEndTime': 1614772645359, 'taskID': 'TOK-1614772577962', 'taskStarttime': 1614772577962, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_TOK'}\n",
            "{'data': '9dfa7990-fc48-462d-b6bb-526a65c8854d.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "9dfa7990-fc48-462d-b6bb-526a65c8854d.txt\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': '9dfa7990-fc48-462d-b6bb-526a65c8854d.txt', 'outputFile': '0-16147802241551387.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'data': '548c97fe-bc30-47c6-83ce-ab958212b299.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "548c97fe-bc30-47c6-83ce-ab958212b299.txt\n",
            "{'output': [{'inputFile': '548c97fe-bc30-47c6-83ce-ab958212b299.txt', 'outputFile': '0-1614780233125228.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS'}\n",
            "{'data': '8d11c741-c8a5-43a5-ab75-ca0fd06cf505.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "8d11c741-c8a5-43a5-ab75-ca0fd06cf505.txt\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': '8d11c741-c8a5-43a5-ab75-ca0fd06cf505.txt', 'outputFile': '0-16147802412201118.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'data': '57000fd0-f285-4f8a-b371-dca376ab0f88.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "57000fd0-f285-4f8a-b371-dca376ab0f88.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': 'f165075c-ee7a-43ba-84ff-72c767f184b5.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "f165075c-ee7a-43ba-84ff-72c767f184b5.txt\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'data': '3c06ec5f-9e1f-422a-8d7d-026f6d42d6aa.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "3c06ec5f-9e1f-422a-8d7d-026f6d42d6aa.txt\n",
            "{'jobID': 'A_TK-ZUceB-1614772577879', 'output': [{'inputFile': '3c06ec5f-9e1f-422a-8d7d-026f6d42d6aa.txt', 'outputFile': '0-16147803778453958.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 0, 'taskEndTime': 1614772645359, 'taskID': 'TOK-1614772577962', 'taskStarttime': 1614772577962, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_TOK'}\n",
            "{'data': 'f91d50ec-9075-4d6d-a2ef-dfe4ef8314b7.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "f91d50ec-9075-4d6d-a2ef-dfe4ef8314b7.txt\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': 'f91d50ec-9075-4d6d-a2ef-dfe4ef8314b7.txt', 'outputFile': '0-16147803894167542.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'data': '0bac256a-026d-4093-8f2a-47696f7e7f35.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "0bac256a-026d-4093-8f2a-47696f7e7f35.txt\n",
            "{'output': [{'inputFile': '0bac256a-026d-4093-8f2a-47696f7e7f35.txt', 'outputFile': '0-16147803957418528.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS'}\n",
            "{'data': 'f613b976-783a-4ae3-be05-2ae93cbaeb86.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "f613b976-783a-4ae3-be05-2ae93cbaeb86.txt\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': 'f613b976-783a-4ae3-be05-2ae93cbaeb86.txt', 'outputFile': '0-1614780406740807.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': '7db5c27d-9eb1-4045-bee5-ed9aad76bc0c.txt', 'outputFile': '0-161478124889987.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': 'cb4b5b68-8fb9-4c62-bf8f-d9cdb9a0f0a8.txt', 'outputFile': '0-16147817898462968.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'jobID': 'A_FBTTR-BHLVk-1614766956800', 'output': [{'inputFile': 'e36e793c-eac3-4c1e-865c-5e53e0266c2b.txt', 'outputFile': '0-1614781794201301.txt', 'outputLocale': 'ta', 'outputType': 'txt'}], 'state': 'SENTENCE-TOKENISED', 'status': 'SUCCESS', 'stepOrder': 2, 'taskEndTime': 1614768649096, 'taskID': 'TOK-1614768646979', 'taskStarttime': 1614768646979, 'tool': 'TOKENISER', 'workflowCode': 'WF_A_FCBMTKTR'}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n",
            "{'Errors': [{'code': 'ZuulRuntimeException', 'message': 'com.netflix.zuul.exception.ZuulException: Forwarding error'}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zGTgEPYP6_T"
      },
      "source": [
        "#### Clean up failed setence segmentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMCFC3f4P6_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be774fcd-960a-46a4-9ad8-118034357cf5"
      },
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "present = defaultdict(lambda :0)\n",
        "for a in glob.glob(sentences_dir+\"/*/*\"):\n",
        "    b = a.split(\"/\")[-2]\n",
        "    present[b] = present[b]+1\n",
        "\n",
        "to_cleanup = []\n",
        "for key,value in present.items():\n",
        "#     print(key,value)\n",
        "    if value != 2:\n",
        "        to_cleanup.append(key)\n",
        "        print(key)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "47\n",
            "43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bRbAZv8P6_T"
      },
      "source": [
        "for a in glob.glob(sentences_dir+\"/*\"):\n",
        "    if any(i in a for i in to_cleanup):\n",
        "        shutil.rmtree(a)\n",
        "#         print(a)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDZv_wWP6_T"
      },
      "source": [
        "### Sentence Alignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgd2kTpNP6_T"
      },
      "source": [
        "upload_url = \"https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file\"\n",
        "aligner_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "download_url = \"https://auth.anuvaad.org/download/\"\n",
        "search_url = \"'https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/jobs/search/bulk'\"\n",
        "\n",
        "# input_path = \"/Users/eaxxkra/Downloads/tn_budget_sentences/\"\n",
        "# output_path = \"/Users/eaxxkra/Downloads/tn_budget_aligned_sentences/\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgKxr-rBP6_T",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d429242d-1dfc-4ecc-e733-79042d722f21"
      },
      "source": [
        "def bulk_search_alignment(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    prev_progress = \"\"\n",
        "   \n",
        "    while(1):\n",
        "        \n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "       \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            data = res.json()\n",
        "            secondlanguage = data['jobs'][0]['input']['files'][0]['locale']    \n",
        "            print(secondlanguage)\n",
        "            source , target = \"source\" , \"target\"\n",
        "            if secondlanguage == 'en.txt':\n",
        "                source , target = target, source\n",
        "            nomatch=str(data['jobs'][0]['output']['noMatch']['source'])\n",
        "            match_english=str(data['jobs'][0][\"output\"]['match'][target])\n",
        "            match_non_english=str(data['jobs'][0][\"output\"]['match'][source])\n",
        "            almostmatch_english=str(data['jobs'][0]['output']['almostMatch'][target])\n",
        "            almostatch_non_english=str(data['jobs'][0]['output']['almostMatch'][source])\n",
        "            return {\n",
        "                'match_english' : match_english,\n",
        "                'match_non_english' : match_non_english,\n",
        "                'almost_match_english' : almostmatch_english,\n",
        "                'almost_non_match_english' : almostatch_non_english,\n",
        "                'no_match' : nomatch}\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        if progress != prev_progress:\n",
        "            print(progress)\n",
        "            prev_progress = progress\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "        #print(res.json())\n",
        "\n",
        "def upload_files(folder):\n",
        "    path_lang_tup = []\n",
        "    for file in glob.glob(folder+\"/*\"):\n",
        "        print(file)\n",
        "        lang = file.split(\"_\")[-1]        \n",
        "        path_lang_tup.append((upload_file(file),lang))\n",
        "    return path_lang_tup\n",
        "\n",
        "def submit_alignment_job(path_lang_tup):\n",
        "    print(path_lang_tup)\n",
        "    files = []\n",
        "    for path, lang in path_lang_tup:\n",
        "        files.append({\n",
        "                        \"locale\": lang,\n",
        "                        \"path\": path,\n",
        "                        \"type\": \"txt\"\n",
        "                    })\n",
        "    \n",
        "    \n",
        "    aligner_body = {\n",
        "        \"workflowCode\":\"WF_A_AL\",\n",
        "        \"files\": files}\n",
        "    \n",
        "    aligner_response = requests.request(\"POST\", aligner_url, json=aligner_body, headers=headers).json()\n",
        "    return aligner_response['jobID']\n",
        "    \n",
        "jobs_to_track = []\n",
        "for folder in glob.glob(sentences_dir+\"/*\"):\n",
        "    print(folder)\n",
        "    path_lang_tup = upload_files(folder)\n",
        "    job_id = submit_alignment_job(path_lang_tup)\n",
        "    jobs_to_track.append((job_id,folder.split(\"/\")[-1]))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/26\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/26/en_26_ta.txt\n",
            "{'data': 'f24f39b7-efb7-4397-a44c-0aa913141837.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/26/ta_26_ta.txt\n",
            "{'data': '3eab5d4d-3d45-444e-a773-d58f0b1c5667.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "[('f24f39b7-efb7-4397-a44c-0aa913141837.txt', 'ta.txt'), ('3eab5d4d-3d45-444e-a773-d58f0b1c5667.txt', 'ta.txt')]\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/44\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/44/ta_44_ta.txt\n",
            "{'data': '5f85c32c-0d5a-4d54-8da6-7d7d9a366e18.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/44/en_44_ta.txt\n",
            "{'data': '40db70ec-761a-4723-91ae-d90d6fb24495.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "[('5f85c32c-0d5a-4d54-8da6-7d7d9a366e18.txt', 'ta.txt'), ('40db70ec-761a-4723-91ae-d90d6fb24495.txt', 'ta.txt')]\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/37\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/37/ta_37_ta.txt\n",
            "{'data': '7ef52939-b3fb-4c79-b081-67998ba5e69c.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/37/en_37_ta.txt\n",
            "{'data': '1236801f-097d-4fbc-b00f-45a63b7d0a0d.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "[('7ef52939-b3fb-4c79-b081-67998ba5e69c.txt', 'ta.txt'), ('1236801f-097d-4fbc-b00f-45a63b7d0a0d.txt', 'ta.txt')]\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/36\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/36/ta_36_ta.txt\n",
            "{'data': '1cce9c5e-d2ba-45a9-8e2e-47c44fc3ec4f.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/36/en_36_ta.txt\n",
            "{'data': '2cdc5403-f518-4577-9bba-276a4d74e97c.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "[('1cce9c5e-d2ba-45a9-8e2e-47c44fc3ec4f.txt', 'ta.txt'), ('2cdc5403-f518-4577-9bba-276a4d74e97c.txt', 'ta.txt')]\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/34\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/34/ta_34_ta.txt\n",
            "{'data': 'adfdf022-b115-4d41-9892-b756eea2e59e.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/34/en_34_ta.txt\n",
            "{'data': '87be61bc-2f6c-4e62-b391-6b791d64ad96.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "[('adfdf022-b115-4d41-9892-b756eea2e59e.txt', 'ta.txt'), ('87be61bc-2f6c-4e62-b391-6b791d64ad96.txt', 'ta.txt')]\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/33\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/33/ta_33_ta.txt\n",
            "{'data': '7d7c4a9d-0871-4150-bbe9-5e3100bf9c9c.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/33/en_33_ta.txt\n",
            "{'data': '81bab864-fcb2-4273-b0e3-5a6eeb97f9eb.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "[('7d7c4a9d-0871-4150-bbe9-5e3100bf9c9c.txt', 'ta.txt'), ('81bab864-fcb2-4273-b0e3-5a6eeb97f9eb.txt', 'ta.txt')]\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/30\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/30/ta_30_ta.txt\n",
            "{'data': 'a81bab3f-4724-446d-8f29-c7c7858b36e7.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/30/en_30_ta.txt\n",
            "{'data': '92d55db5-fc9a-44ef-a485-f8858ece049d.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "[('a81bab3f-4724-446d-8f29-c7c7858b36e7.txt', 'ta.txt'), ('92d55db5-fc9a-44ef-a485-f8858ece049d.txt', 'ta.txt')]\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/27\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/27/ta_27_ta.txt\n",
            "{'data': 'b7a5e668-9a63-4836-b20e-1d7396d225dd.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/27/en_27_ta.txt\n",
            "{'data': 'c965f044-ad8a-4152-a0b4-704f4f5454f2.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "[('b7a5e668-9a63-4836-b20e-1d7396d225dd.txt', 'ta.txt'), ('c965f044-ad8a-4152-a0b4-704f4f5454f2.txt', 'ta.txt')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ0rFzE3P6_U",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d224590-5fa1-4660-c64f-0db5f6822eda"
      },
      "source": [
        "\n",
        "for job_id, folder in jobs_to_track:\n",
        "    file_to_download = bulk_search_alignment(job_id,bs_url,headers)\n",
        "    for key , value in file_to_download.items():\n",
        "        content = download_file(value)\n",
        "        os.system('mkdir -p \"{}\"'.format(aligned_sentences_path+folder))\n",
        "        with open(os.path.join(aligned_sentences_path,folder,f\"{key}.text\"),'w',encoding=\"utf-16\") as f:\n",
        "            f.writelines(content.decode('utf-16'))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A_A-VywVQ-1614782386869\n",
            "COMPLETED\n",
            "ta.txt\n",
            "A_A-mCBEf-1614782392248\n",
            "INPROGRESS\n",
            "COMPLETED\n",
            "ta.txt\n",
            "A_A-rTChI-1614782398067\n",
            "COMPLETED\n",
            "ta.txt\n",
            "A_A-LGcbH-1614782403435\n",
            "COMPLETED\n",
            "ta.txt\n",
            "A_A-HQWpr-1614782408835\n",
            "COMPLETED\n",
            "ta.txt\n",
            "A_A-fKNww-1614782414201\n",
            "COMPLETED\n",
            "ta.txt\n",
            "A_A-YKvNq-1614782420353\n",
            "INPROGRESS\n",
            "COMPLETED\n",
            "ta.txt\n",
            "A_A-fhRzg-1614782426806\n",
            "INPROGRESS\n",
            "COMPLETED\n",
            "ta.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twRv1icrP6_U"
      },
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "present = defaultdict(lambda :0)\n",
        "for a in glob.glob(aligned_sentences_path+\"/*/*\"):\n",
        "    b = a.split(\"/\")[-2]\n",
        "    present[b] = present[b]+1\n",
        "\n",
        "to_cleanup = []\n",
        "for key,value in present.items():\n",
        "#     print(key,value)\n",
        "    if value != 5:\n",
        "        to_cleanup.append(key)\n",
        "        print(key)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKoPg3VwP6_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ef1ad5-a3f6-46a7-9e2f-bbba12bd0c88"
      },
      "source": [
        "to_cleanup"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYMkY1btP6_U"
      },
      "source": [
        "### Aligned Sentences to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0CWhibjP6_U"
      },
      "source": [
        "import pandas as pd\n",
        "match_dfs = []\n",
        "almost_match_dfs = []\n",
        "for a in glob.glob(aligned_sentences_path+\"/*\"):\n",
        "    match = pd.DataFrame()\n",
        "    with open(a + \"/\" + \"match_english.text\",encoding='utf-16') as f:\n",
        "        match['english'] = f.readlines()\n",
        "    with open(a + \"/\" + \"match_non_english.text\",encoding='utf-16') as f:\n",
        "        match['non_english'] = f.readlines()\n",
        "    match_dfs.append(match)\n",
        "    almost_match = pd.DataFrame()\n",
        "    with open(a + \"/\" + \"almost_match_english.text\",encoding='utf-16') as f:\n",
        "        almost_match['english'] = f.readlines()\n",
        "    with open(a + \"/\" + \"almost_non_match_english.text\",encoding='utf-16') as f:\n",
        "        almost_match['non_english'] = f.readlines()\n",
        "    match_dfs.append(match)\n",
        "    almost_match_dfs.append(almost_match)\n",
        "\n",
        "match = pd.concat(match_dfs)\n",
        "almost_match = pd.concat(almost_match_dfs)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A0nsCVFP6_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29947796-0688-4503-afff-f1e30505dcce"
      },
      "source": [
        "len(match)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1480"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT7UfJFoP6_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4919cd1-68e5-4e67-bf10-d5104a6a771c"
      },
      "source": [
        "len(almost_match)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJNqh6uVP6_V"
      },
      "source": [
        "match.to_csv(parent_folder + \"match.csv\")\n",
        "almost_match.to_csv(parent_folder + \"almost_match.csv\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D26zBJUGP6_V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}