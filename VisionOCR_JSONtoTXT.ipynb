{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PIB py3.7",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "VisionOCR_JSONtoTXT.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/AI4BharatTranslation/blob/main/VisionOCR_JSONtoTXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf-DvL37rHv3"
      },
      "source": [
        "import glob\n",
        "import uuid\n",
        "import json\n",
        "import requests\n",
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "import logging"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69nIAfPQrHv8"
      },
      "source": [
        "path = '/content/'\n",
        "output_path = '/content/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX3__tWPrHv9"
      },
      "source": [
        "language = 'detect'\n",
        "file_format = 'PDF'\n",
        "\n",
        "evaluation_level = 'LINE'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlqruzoBrHv-"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opnGysXgrHv-"
      },
      "source": [
        "token ='eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyTmFtZSI6ImhhcnNoaXRhZGRAZ21haWwuY29tIiwicGFzc3dvcmQiOiJiJyQyYiQxMiR2L25RbDQ0dWZHdXhoUEJpZElHYm11S2hMcHVWLmhwQWs5cTF6M2UvNkoueXltUEJnUFNkTyciLCJleHAiOjE2MTQ0MDAzNDN9.eNtXbLuufgaU3q69mUSDxSjXmmcs3FO9s31VurJ5TIE'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QKHsXZErHv_"
      },
      "source": [
        "#path = '/home/srihari/Desktop/data/data'\n",
        "word_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "google_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "layout_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "segmenter_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "bs_url =\"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/jobs/search/bulk\"\n",
        "\n",
        "evaluator_url  = \"https://auth.anuvaad.org/anuvaad-etl/document-processor/evaluator/v0/process\"\n",
        "\n",
        "#evaluator_url = 'http://0.0.0.0:5001/anuvaad-etl/document-processor/evaluator/v0/process'\n",
        "\n",
        "download_url =\"https://auth.anuvaad.org/download/\"\n",
        "upload_url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "\n",
        "\n",
        "headers = {\n",
        "    'auth-token' :token }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZf7jegorHwA"
      },
      "source": [
        "class Draw:\n",
        "    \n",
        "    def __init__(self,input_json,save_dir,regions,prefix='',color= (255,0,0),thickness=5):   \n",
        "        self.json = input_json\n",
        "        self.save_dir = save_dir\n",
        "        self.regions = regions\n",
        "        self.prefix  = prefix\n",
        "        self.color  = color\n",
        "        self.thickness=thickness\n",
        "        if self.prefix in ['seg','tess']:\n",
        "            #print('drawing children')\n",
        "            self.draw_region_children()\n",
        "        else:\n",
        "            self.draw_region()\n",
        "        \n",
        "    def get_coords(self,page_index):\n",
        "        return self.json['outputs'][0]['pages'][page_index][self.regions]\n",
        "    \n",
        "    def get_page_count(self):\n",
        "        return(self.json['outputs'][0]['page_info'])\n",
        "    \n",
        "    def get_page(self,page_index):\n",
        "        page_path = self.json['outputs'][0]['page_info'][page_index]\n",
        "        page_path = page_path.split('upload')[1]#'/'.join(page_path.split('/')[1:])\n",
        "        print(page_path)    \n",
        "        return download_file(download_url,headers,page_path,f_type='image')\n",
        "\n",
        "    def draw_region(self):\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "        for page_index in range(len(self.get_page_count())) :\n",
        "            nparr = np.fromstring(self.get_page(page_index), np.uint8)\n",
        "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "            for region in self.get_coords(page_index) :\n",
        "                    ground = region['boundingBox']['vertices']\n",
        "                    pts = []\n",
        "                    for pt in ground:\n",
        "                        pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "                    cv2.polylines(image, [np.array(pts)],True, self.color, self.thickness)\n",
        "                    cv2.putText(image, str(region['class']), (pts[0][0],pts[0][1]), font,  \n",
        "                   2, (0,125,255), 3, cv2.LINE_AA)\n",
        "                    \n",
        "            image_path = os.path.join(self.save_dir ,  '{}_{}_{}.png'.format(self.regions,self.prefix,page_index))            \n",
        "            cv2.imwrite(image_path , image)\n",
        "          \n",
        "    def draw_region_children(self):\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "        fontScale = 2\n",
        "        thickness =3\n",
        "\n",
        "\n",
        "        for page_index in range(len(self.get_page_count())) :\n",
        "            nparr = np.fromstring(self.get_page(page_index), np.uint8)\n",
        "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "            for region_index,region in enumerate(self.get_coords(page_index)) :\n",
        "                try:\n",
        "                    ground = region['boundingBox']['vertices']\n",
        "                    pts = []\n",
        "                    for pt in ground:\n",
        "                        pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "                    #print(pts)\n",
        "                    region_color = (0 ,0,125+ 130*(region_index/ len(self.get_coords(page_index))))\n",
        "                    cv2.polylines(image, [np.array(pts)],True, region_color, self.thickness)\n",
        "                    cv2.putText(image, str(region_index), (pts[0][0],pts[0][1]), font,  \n",
        "                   fontScale, region_color, thickness, cv2.LINE_AA)\n",
        "                    for line_index, line in enumerate(region['children']):\n",
        "                        ground = line['boundingBox']['vertices']\n",
        "                        pts = []\n",
        "                        for pt in ground:\n",
        "                            pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "\n",
        "                        line_color = (125 + 130*(region_index/ len(self.get_coords(page_index))) ,0,0)\n",
        "                        cv2.polylines(image, [np.array(pts)],True, line_color, self.thickness -2)\n",
        "                        cv2.putText(image, str(line_index), (pts[0][0],pts[0][1]), font,  \n",
        "                   fontScale, line_color, thickness, cv2.LINE_AA)\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "                    print(region)\n",
        "                    \n",
        "            image_path = os.path.join(self.save_dir ,  '{}_{}.png'.format(self.prefix,page_index))\n",
        "            cv2.imwrite(image_path , image)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbIQtwW9rHwB"
      },
      "source": [
        "def draw_region(image,regions,color,key):\n",
        "    nparr = np.fromstring(image, np.uint8)\n",
        "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    for region in regions :\n",
        "            ground = region[key]['boundingBox']['vertices']\n",
        "            pts = []\n",
        "            for pt in ground:\n",
        "                pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "            cv2.polylines(image, [np.array(pts)],True,color, 3)\n",
        "\n",
        "    return image\n",
        "   \n",
        "\n",
        "def draw_compare(image,regions):\n",
        "    nparr = np.fromstring(image, np.uint8)\n",
        "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    for region in regions :\n",
        "        if region['ground'] != None :\n",
        "            ground = region['ground']['boundingBox']['vertices']\n",
        "            pts = []\n",
        "            for pt in ground:\n",
        "                pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "            cv2.polylines(image, [np.array(pts)],True,(255,0,0), 3)\n",
        "\n",
        "        if region['input'] != None :\n",
        "            inpu_t = region['input']['boundingBox']['vertices']\n",
        "            pts = []\n",
        "            for pt in inpu_t:\n",
        "                pts.append([int(pt['x']) ,int(pt['y'])])\n",
        "            cv2.polylines(image, [np.array(pts)],True,(0,0,255), 3)         \n",
        "    return image\n",
        "    \n",
        "    \n",
        "      \n",
        "def draw_erros(page_row,page_index):\n",
        "    page_path = page_row['path'].split('upload')[1]\n",
        "    image = download_file(download_url,headers,page_path,f_type='image')\n",
        "    not_in_pred = draw_region(image,page_row['only_in_ground'],(255,0,0),'ground')\n",
        "    not_in_ground = draw_region(image,page_row['only_in_pred'],(0,0,255),'input' )\n",
        "    compare       = draw_compare(image,page_row['below_iou_threshold'])\n",
        "    return np.hstack([not_in_pred,compare,not_in_ground])\n",
        "\n",
        "        \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg0GhvJSrHwC"
      },
      "source": [
        "\n",
        "def find_erros(eval_data,input_data,iou_threshold):\n",
        "  \n",
        "    count_erros = []\n",
        "    iou_error   = []\n",
        "    for file_index in range(len(eval_data['rsp']['outputs'])) :\n",
        "        file_name = eval_data['rsp']['input']['inputs'][file_index]['input']['name']\n",
        "        \n",
        "        for page_index in range(len(eval_data['rsp']['outputs'][file_index]['pages'])):\n",
        "           \n",
        "    \n",
        "            print('evaluationg file {} page {}'.format(file_index,page_index))\n",
        "\n",
        "            page = eval_data['rsp']['outputs'][file_index]['pages'][page_index]\n",
        "            page_image = input_data['outputs'][file_index]['page_info'][page_index]\n",
        "\n",
        "            regions = {}\n",
        "            #try :\n",
        "            counts = page['count'] \n",
        "            count_diff  = abs(counts['input'] - counts['ground'])\n",
        "            #print('diff in boxes is {}'.format(count_diff))\n",
        "\n",
        "           # if count_diff >= count_threshold :\n",
        "                    #count_erros.append({'file_index' : file_index ,'page_index': page_index,'page':page,'path':page_image})\n",
        "            avg_iou = pd.DataFrame(page['iou'])['iou'].mean()\n",
        "            print('avrage page iou is {} \\n'.format(pd.DataFrame(page['iou'])['iou'].mean()))\n",
        "            #regions = {'below_iou_threshold':[],'avg_iou': avg_iou, 'count_diff':count_diff,'count_input':counts['input'],'count_ground':counts['ground'], 'file_index' : file_index ,'page_index': page_index, 'page':page['iou'],'path':page_image}\n",
        "            regions = {'below_iou_threshold':[],\\\n",
        "                       'avg_iou': avg_iou,'count_input':counts['input'],\\\n",
        "                       'count_ground':counts['ground'], \\\n",
        "                       'file_name' : file_name,\\\n",
        "                       'file_index' : file_index ,'page_index': page_index,\\\n",
        "                       'page':page['iou'],\\\n",
        "                       'only_in_ground' : [],\\\n",
        "                       'only_in_pred'    : [],\n",
        "                       'path':page_image}\n",
        "\n",
        "\n",
        "            for region_iou in page['iou']:\n",
        "                if region_iou['iou'] <= iou_threshold:\n",
        "                    regions['below_iou_threshold'].append(region_iou)\n",
        "\n",
        "            regions['count_below_iou_threshold']= len(regions['below_iou_threshold'])\n",
        "\n",
        "            for region_iou in page['iou']:\n",
        "                if region_iou['ground'] is None:\n",
        "                    regions['only_in_pred'].append(region_iou)\n",
        "\n",
        "                if region_iou['input'] is None :\n",
        "                    regions['only_in_ground'].append(region_iou)\n",
        "\n",
        "            regions['count_below_iou_threshold']= len(regions['below_iou_threshold'])\n",
        "            #iou_error.append(regions)\n",
        "\n",
        "\n",
        "            #except Exception as e :\n",
        "            #    print(e)\n",
        "            \n",
        "            iou_error.append(regions)\n",
        "            \n",
        "    return pd.DataFrame(iou_error)\n",
        "    \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zydUVGGyrHwE"
      },
      "source": [
        "def word_detector(word_url,headers,pdf_name):\n",
        "    #pdf_name = '/home/srihari/Downloads/uploads/13147_2018_6_15_11910_Judgement_29-Jan-2019_ASM.pdf'\n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": pdf_name,\n",
        "            \"type\": file_format,\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_WD\"\n",
        "    }\n",
        "    res = requests.post(word_url,json=file,headers=headers)\n",
        "    return res.json()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmNAnT6crHwG"
      },
      "source": [
        "def google_ocr(word_url,headers,pdf_name):\n",
        "    \n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": pdf_name,\n",
        "            \"type\": file_format,\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_OGV\"\n",
        "    }\n",
        "    res = requests.post(word_url,json=file,headers=headers)\n",
        "    return res.json()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYZ3U9B9rHwH"
      },
      "source": [
        "def tesseract_ocr(layout_url,headers,layout_detector_output,language=language):\n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": layout_detector_output,\n",
        "            \"type\": \"json\",\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_OTES\"\n",
        "    }\n",
        "    res = requests.post(layout_url,json=file,headers=headers)\n",
        "    return res.json()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQHIthsUrHwI"
      },
      "source": [
        "def layout_detector(layout_url,headers,word_detector_output):\n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": word_detector_output,\n",
        "            \"type\": \"json\",\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_LD\"\n",
        "    }\n",
        "    res = requests.post(layout_url,json=file,headers=headers)\n",
        "    return res.json()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExeqaOV9rHwI"
      },
      "source": [
        "def block_segmenter(segmenter_url,headers,layout_detector_output):\n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": layout_detector_output,\n",
        "            \"type\": \"json\",\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_BS\"\n",
        "    }\n",
        "    res = requests.post(layout_url,json=file,headers=headers)\n",
        "    return res.json()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVXQp9wjrHwJ"
      },
      "source": [
        "def evaluator(evaluator_url,headers,input_json):\n",
        "    file = {\n",
        "                \"input\": { \"inputs\": [\n",
        "                {\n",
        "                  \"input\": {\n",
        "                    \"jobId\": \"string\",\n",
        "                    \"name\": input_json['predicted'],\n",
        "                    \"type\": \"json\"\n",
        "                  },\n",
        "                  \"ground\": {\n",
        "                    \"jobId\": \"string\",\n",
        "                    \"name\": input_json['ground'],\n",
        "                    \"type\": \"json\"\n",
        "                  },\n",
        "                  \"config\": {\n",
        "                    \"strategy\": \"IOU\",\n",
        "                    \"boxLevel\": input_json['level']\n",
        "                  }\n",
        "                } ]}\n",
        "\n",
        "                }\n",
        "    \n",
        "    res = requests.post(evaluator_url,json=file)\n",
        "    \n",
        "    \n",
        "    return res.json()\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxTSR7AHrHwJ"
      },
      "source": [
        "def upload_file(pdf_file,headers,url):\n",
        "    #url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "    files = [\n",
        "        ('file',(open(pdf_file,'rb')))] \n",
        "\n",
        "    response = requests.post(url, headers=headers, files=files)\n",
        "    \n",
        "    return response.json()\n",
        "    response.json()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL7NCHGDrHwJ"
      },
      "source": [
        "# upload_file(\"/Users/eaxxkra/Downloads/tn_budget_text/GA_2020_ENG_en.txt\",headers,upload_url)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bndFrsYZrHwK"
      },
      "source": [
        "def download_file(download_url,headers,outputfile,f_type='json'):\n",
        "    download_url =download_url+str(outputfile)\n",
        "    res = requests.get(download_url,headers=headers)\n",
        "    if f_type == 'json':\n",
        "        return res.json()\n",
        "    else :\n",
        "        return res.content"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC4cXeaOrHwK"
      },
      "source": [
        "def save_json(path,res):\n",
        "    with open(path, \"w\", encoding='utf8') as write_file:\n",
        "        json.dump(res, write_file,ensure_ascii=False )\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pZF7bl-rHwL"
      },
      "source": [
        "def bulk_search(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    print(res.json())\n",
        "    prev_progress = \"\"\n",
        "   \n",
        "    while(1):\n",
        "        \n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "       \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']\n",
        "            print(outputfile)\n",
        "            print(job_id)\n",
        "            return outputfile\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        if progress != prev_progress:\n",
        "            print(progress)\n",
        "            prev_progress = progress\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "        #print(res.json())\n",
        "      \n",
        "   "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPdUqCOjrHwL"
      },
      "source": [
        "def bulk_search_all(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    print(res.json())\n",
        "    \n",
        "   \n",
        "    while(1):\n",
        "        \n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "       \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']\n",
        "            print(outputfile)\n",
        "            print(job_id)\n",
        "            return outputfile\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        print(progress)\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSUFzSmsrHwM"
      },
      "source": [
        "\n",
        "def execute_module(module,url,input_file,module_code,pdf_dir,overwirte=True , draw=True):\n",
        "    \n",
        "        \n",
        "        \n",
        "        output_path = os.path.join(pdf_dir,'{}.json'.format(module_code))\n",
        "        if os.path.exists(output_path) and not overwirte:\n",
        "            print(' loading *****************{}'.format(module_code ))\n",
        "            with open(output_path,'r') as wd_file :\n",
        "                response = json.load(wd_file)\n",
        "                \n",
        "            wf_res = pdf_dir + '/{}_wf.json'.format(module_code)\n",
        "            with open(wf_res,'r') as wd_file :\n",
        "                json_file = json.load(wd_file) \n",
        "            #json_file = upload_file(output_path,headers,upload_url)['data']\n",
        "            job_to_add = (pdf_dir)\n",
        "\n",
        "        else :\n",
        "            if module_code in ['wd','gv']:\n",
        "                res = upload_file(input_file,headers,upload_url)\n",
        "                print('upload response **********', res)\n",
        "                pdf_name = res['data']\n",
        "                response = module(url,headers,pdf_name)\n",
        "            \n",
        "            else : \n",
        "                response = module(url,headers,input_file)\n",
        "                \n",
        "                if 'eval' in module_code :\n",
        "                    json_file = response['outputFile']\n",
        "                    response = download_file(download_url,headers,json_file)\n",
        "                    save_json(output_path,response)\n",
        "                    return json_file,response\n",
        "                \n",
        "            \n",
        "            print(' response *****************{} {}'.format(module_code ,response ))\n",
        "            job_id = response['jobID']\n",
        "            \n",
        "            job_to_add = (job_id,bs_url,headers,pdf_dir,module_code,output_path)\n",
        "#             json_file = bulk_search(job_id,bs_url,headers)\n",
        "#             save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   \n",
        "#             print('bulk search  response **************',json_file )\n",
        "#             response = download_file(download_url,headers,json_file)\n",
        "#             save_json(output_path,response)\n",
        "            \n",
        "#             if draw :\n",
        "#                 if module_code in ['wd','gv']:\n",
        "#                     Draw(response,pdf_dir,regions='lines',prefix=module_code)\n",
        "#                 else :\n",
        "#                      Draw(response,pdf_dir,regions='regions',prefix=module_code)\n",
        "        return job_to_add\n",
        "#         return json_file,response\n",
        "        \n",
        "            \n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVG_2k6MrHwM"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzm8oi-yrHwN"
      },
      "source": [
        "def evaluate__and_save_input(pdf_files,output_dir,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
        "    word_responses   = {}\n",
        "    layout_responses = {}\n",
        "    segmenter_responses = []\n",
        "    jobs_to_track = []\n",
        "    for pdf in pdf_files:\n",
        "        try :\n",
        "            pdf_name = pdf.split('/')[-1].split('.')[0]\n",
        "            print(pdf , ' is being processed')\n",
        "            pdf_output_dir = os.path.join(output_dir,pdf_name)\n",
        "            os.system('mkdir -p \"{}\"'.format(pdf_output_dir))\n",
        "\n",
        "\n",
        "#             wd_json,_ = execute_module(word_detector,word_url,input_file=pdf,\\\n",
        "#                            module_code='wd',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "#             google_json,google_resposne = \n",
        "            job_to_add = execute_module(google_ocr,google_url,input_file=pdf,\\\n",
        "                           module_code='gv',pdf_dir=pdf_output_dir,overwirte=False , draw=False)\n",
        "            \n",
        "            jobs_to_track.append(job_to_add)\n",
        "#             ld_json,_ = execute_module(layout_detector,layout_url,input_file=wd_json,\\\n",
        "#                            module_code='ld',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "#             seg_json,_ = execute_module(block_segmenter,segmenter_url,input_file=ld_json,\\\n",
        "#                            module_code='seg',pdf_dir=pdf_output_dir,overwirte=False , draw=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#             tess_json,_= execute_module(tesseract_ocr,word_url,input_file=seg_json,\\\n",
        "#                         module_code='tess',pdf_dir=pdf_output_dir,overwirte=True , draw=True)\n",
        "\n",
        "\n",
        "#             evaluator_input= {'predicted': tess_json , 'ground':google_json ,'level': evaluation_level}\n",
        "\n",
        "#             eval_json ,evaluator_response = execute_module(evaluator,evaluator_url,\\\n",
        "#                                                            input_file=evaluator_input,module_code='eval_line', \\\n",
        "#                                                            pdf_dir= pdf_output_dir)\n",
        "\n",
        "\n",
        "#             error_df = find_erros(evaluator_response,google_resposne,iou_threshold=0.33)\n",
        "#             #error_df\n",
        "#             for page_index,page_row in error_df.iterrows():\n",
        "#                 img = draw_erros(page_row,page_index)\n",
        "#                 cv2.imwrite('{}/eval_{}_{}.png'.format(pdf_output_dir,evaluation_level,page_index),np.array(img).astype('uint8'))\n",
        "        \n",
        "\n",
        "          \n",
        "        except Exception as e : \n",
        "            print(e)\n",
        "            logging.error('error in file {}  \\n {}'.format(pdf_name,e))\n",
        "\n",
        " \n",
        "    for job in jobs_to_track:\n",
        "        print(\"----------\")\n",
        "        print(job)\n",
        "        job_id,bs_url,headers,pdf_dir,module_code,output_path = job\n",
        "        json_file = bulk_search(job_id,bs_url,headers)\n",
        "        save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   \n",
        "        print('bulk search  response **************',json_file )\n",
        "        response = download_file(download_url,headers,json_file)\n",
        "        save_json(output_path,response)\n",
        "\n",
        "    print(\"----------------\")\n",
        "    print(jobs_to_track)\n",
        "    print(\"----------------\")\n",
        "    return jobs_to_track    \n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxSPtkNJrHwN"
      },
      "source": [
        "def main(path,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
        "        pdf_names = glob.glob(path + '/*.pdf')\n",
        "        pdf_names.reverse()\n",
        "        \n",
        "        return evaluate__and_save_input(pdf_names,output_path,headers,word_url,layout_url,download_url,upload_url,bs_url)\n",
        "        "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "83ttwVpTrHwO"
      },
      "source": [
        "main(path,headers,word_url,layout_url,download_url,upload_url,bs_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44qQKhRotWs8"
      },
      "source": [
        "folders = os.listdir(path)\r\n",
        "for folder in folders: \r\n",
        "  if 'te' in folder:\r\n",
        "    fn = path + folder + '/gv.json'\r\n",
        "    if '.pdf' not in fn:\r\n",
        "      with open(fn,'r') as f:\r\n",
        "        data = json.load(f)\r\n",
        "        pages = data['outputs'][0]['pages']\r\n",
        "        all_texts = []\r\n",
        "        for page in pages:\r\n",
        "            page_text = []\r\n",
        "            lines = page['lines']\r\n",
        "            for line in lines:\r\n",
        "                page_text.append(line['text'])\r\n",
        "            all_texts.append(page_text)\r\n",
        "        all_lines = [line for al in all_texts for line in al]\r\n",
        "        all_text = \"\\n\".join(all_lines)\r\n",
        "      f.close()\r\n",
        "      outfn = path + folder +'.txt'\r\n",
        "      with open(outfn,'w') as f:\r\n",
        "        f.write(all_text)\r\n",
        "      f.close()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcmveKT3rW4_"
      },
      "source": [
        "def extract_text(folder_name):\n",
        "    with open(f\"/Users/eaxxkra/Downloads/sample_out/{folder_name}/gv.json\",'r') as f:\n",
        "        data = json.load(f)\n",
        "    pages = data['outputs'][0]['pages']\n",
        "    all_texts = []\n",
        "    for page in pages:\n",
        "        page_text = []\n",
        "        lines = page['lines']\n",
        "        for line in lines:\n",
        "            page_text.append(line['text'])\n",
        "        all_texts.append(page_text)\n",
        "    all_lines = [line for al in all_texts for line in al]\n",
        "    all_text = \" \".join(all_lines)\n",
        "    lang = folder_name[-2:]\n",
        "    sentences = \"\\n\".join(sent_tokenize(all_text,language=f\"{lang}.pib.news\"))\n",
        "    with open(f\"/Users/eaxxkra/Downloads/tn_budget_text/{folder_name}.txt\",'w') as f:\n",
        "        f.writelines(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3aY0nltrW4_"
      },
      "source": [
        "import glob\n",
        "for name in glob.glob('/Users/eaxxkra/Downloads/sample_out/*'):\n",
        "    folder_name = name.replace(\"/Users/eaxxkra/Downloads/sample_out/\",\"\")\n",
        "#     print(folder_name)\n",
        "    try:\n",
        "        extract_text(folder_name)\n",
        "    except:\n",
        "        print(folder_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9ES0yJxrHwP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}