{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LaBSEAligner_SelfVersion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1MnNLwjksHcOe2om8xNb2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/AI4BharatTranslation/blob/main/LaBSEAligner_SelfVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_SRarMhD5TD"
      },
      "source": [
        "!pip install sentence_transformers\r\n",
        "!pip install scipy\r\n",
        "!pip install sentence-splitter\r\n",
        "!pip install indic-nlp-library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e31UQsBJH0W"
      },
      "source": [
        "import sys\r\n",
        "from indicnlp.tokenize import indic_tokenize\r\n",
        "from indicnlp.tokenize import sentence_tokenize \r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "import numpy as np\r\n",
        "import re \r\n",
        "from sentence_splitter import SentenceSplitter, split_text_into_sentences\r\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AiW7zbjAWiO",
        "outputId": "8aa40a8f-76ff-4f4e-d9fd-1cdc13446fca"
      },
      "source": [
        "model = SentenceTransformer('LaBSE')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.75G/1.75G [01:11<00:00, 24.4MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szQaA6FsOCTr",
        "outputId": "b1ffeb6e-7999-4f61-92cb-6409b51040a6"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBKmprDaOMhU",
        "outputId": "fcb8d039-2da8-43ab-842a-1d55c8733894"
      },
      "source": [
        "with open('/content/drive/MyDrive/eng_2.txt','r') as file:\r\n",
        "  content = file.read()\r\n",
        "content = content.replace('\\n',' ')\r\n",
        "splitter = SentenceSplitter(language='en')\r\n",
        "so_sentences = splitter.split(content)\r\n",
        "print('English Sentence Count is - '+ str(len(so_sentences)))\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/tam_0_1.txt','r') as file:\r\n",
        "  tgtp = file.read()\r\n",
        "ta_sentences = sentence_tokenize.sentence_split(tgtp, lang='ta') \r\n",
        "print('Tamil Sentence Count is - '+ str(len(ta_sentences)))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Sentence Count is - 813\n",
            "Tamil Sentence Count is - 746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orDbzqpd7CAx"
      },
      "source": [
        "def LaBSEembeddings(source, target):\r\n",
        "  '''\r\n",
        "  Generate LABSE embeddings\r\n",
        "  Note: Inputs are array of strings\r\n",
        "  '''           \r\n",
        "  embeddings_input_1 = model.encode(source,show_progress_bar=True)\r\n",
        "  embeddings_input_2 = model.encode(target,show_progress_bar=True)    \r\n",
        "  print(\"LABSE embedding generation finished\")\r\n",
        "  return embeddings_input_1, embeddings_input_2\r\n",
        "  "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qOlbPq98GCh"
      },
      "source": [
        "def match_target_sentence(source_embedding, target_embeddings):\r\n",
        "  '''\r\n",
        "  Calculate cosine similarity using scipy distance method\r\n",
        "  '''\r\n",
        "  distances = distance.cdist(source_embedding, target_embeddings, \"cosine\")[0]\r\n",
        "  min_index = np.argmin(distances)\r\n",
        "  min_distance = 1 - distances[min_index]\r\n",
        "  print(\"Match score: {}\".format(min_distance))\r\n",
        "  if min_distance >= 0.5:\r\n",
        "      return min_index, min_distance, \"MATCH\"\r\n",
        "  else:\r\n",
        "      return min_index, min_distance, \"NOMATCH\"     \r\n",
        "      \r\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GceylYunEIAC"
      },
      "source": [
        "def main():\r\n",
        "  source_embeddings, target_embeddings = LaBSEembeddings(so_sentences, ta_sentences)\r\n",
        "  target_map = {}\r\n",
        "  source_map = {}\r\n",
        "  for s in range(len(source_embeddings)):\r\n",
        "    source_map[s] = so_sentences[s]\r\n",
        "  for t in range(len(target_embeddings)):\r\n",
        "    target_map[t] = ta_sentences[t]\r\n",
        "  aligned_phrases= {}\r\n",
        "  for s in range(len(source_embeddings)) :\r\n",
        "      alignments = match_target_sentence([source_embeddings[s]], target_embeddings)\r\n",
        "      if alignments is not None and alignments[2] is \"MATCH\":\r\n",
        "          aligned_phrases[source_map[s]] =  target_map[alignments[0]]\r\n",
        "      print(\"Aligned Phrases: {}\".format(aligned_phrases))\r\n",
        "  return aligned_phrases   \r\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8y5BE3qEmtQ"
      },
      "source": [
        "aligned_phrases = main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL1cwdpuTDzX",
        "outputId": "74a065ef-793b-42e1-8906-b67e48c965a6"
      },
      "source": [
        "print(len(aligned_phrases))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYeK7_STKZAF"
      },
      "source": [
        "# for key in aligned_phrases:\r\n",
        "#   print(key)\r\n",
        "#   print(aligned_phrases[key])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQqrf2ohTiI2"
      },
      "source": [
        "ap = []\r\n",
        "for key in aligned_phrases:\r\n",
        "  l = []\r\n",
        "  l.append(key)\r\n",
        "  l.append(aligned_phrases[key])\r\n",
        "  ap.append(l)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0e2rBtXKeoI"
      },
      "source": [
        "import csv\r\n",
        "with open('matched.csv', 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerow(['English', 'Tamil'])\r\n",
        "    for a in ap:\r\n",
        "      writer.writerow(a)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL59XX8_-0OA"
      },
      "source": [
        "# source_embeddings_mean = [np.mean(s) for s in source_embeddings ]\r\n",
        "# target_embeddings_mean = [np.mean(s) for s in target_embeddings ]"
      ],
      "execution_count": 43,
      "outputs": []
    }
  ]
}