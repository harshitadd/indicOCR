{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LaBSEAligner_SelfVersion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkGbK/SNkf+xZXJBAlqdzT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/AI4BharatTranslation/blob/main/LaBSEAligner_SelfVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_SRarMhD5TD"
      },
      "source": [
        "!pip install sentence_transformers\r\n",
        "!pip install scipy\r\n",
        "!pip install sentence-splitter\r\n",
        "!pip install indic-nlp-library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e31UQsBJH0W"
      },
      "source": [
        "import sys\r\n",
        "from indicnlp.tokenize import indic_tokenize\r\n",
        "from indicnlp.tokenize import sentence_tokenize \r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "import numpy as np\r\n",
        "import re \r\n",
        "from sentence_splitter import SentenceSplitter, split_text_into_sentences\r\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AiW7zbjAWiO",
        "outputId": "8aa40a8f-76ff-4f4e-d9fd-1cdc13446fca"
      },
      "source": [
        "model = SentenceTransformer('LaBSE')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.75G/1.75G [01:11<00:00, 24.4MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szQaA6FsOCTr",
        "outputId": "b1ffeb6e-7999-4f61-92cb-6409b51040a6"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBKmprDaOMhU",
        "outputId": "c5b0d6da-abb2-44ec-be79-527320827ff9"
      },
      "source": [
        "with open('/content/drive/MyDrive/eng_2.txt','r') as file:\r\n",
        "  content = file.read()\r\n",
        "patterns= ['-+','%',' \\* ','\\d{1}\\.\\d{1}','#','\\d{1}','(\\d{2}#)', '(\\d{3}#)', '(\\d{2}\\.)', '(\\d{3}\\.)']\r\n",
        "for pattern in patterns:\r\n",
        "\tcontent = re.sub(pattern,'',content)\r\n",
        "content = content.replace('\\n',' ')\r\n",
        "splitter = SentenceSplitter(language='en')\r\n",
        "so_sentences = splitter.split(content)\r\n",
        "print('English Sentence Count is - '+ str(len(so_sentences)))\r\n",
        "\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/tam_0_1.txt','r') as file:\r\n",
        "  content = file.read()\r\n",
        "patterns = ['!','}','{','\\*','~','@','/','!  ','=','>>','“;','—',',','&',' \\| ','\\(\\)','\\)','\\(  \\)','\\(','-+','%','[A-Z]+','[a-z]+',' \\* ','#','\\d{1}','(\\d{2}\\.)','(\\d{4}\\.)','(\\d{3}\\.)','(\\d{1}\\.)','(\\d{2}#)', '(\\d{3}#)','\\d{4}','\\d{3}','\\d{2}','\\d{4,}','\\[','\\]','\\[\\]' ] #CHANGE THIS IF THE LANGUAGE USES . as delimeter - For languages which have Poorna Virama as delimet\r\n",
        "for pattern in patterns:\r\n",
        "\tcontent = re.sub(pattern,' ',content)\r\n",
        "content = content.replace('\\n', '  ')\r\n",
        "content = content.replace('|','।')\r\n",
        "content = re.sub('( \\.)',' ',content)\r\n",
        "content = re.sub('( . . .)+',' ',content)\r\n",
        "\r\n",
        "ta_sentences = sentence_tokenize.sentence_split(content, lang='ta') \r\n",
        "print('Tamil Sentence Count is - '+ str(len(ta_sentences)))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Sentence Count is - 716\n",
            "Tamil Sentence Count is - 743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orDbzqpd7CAx"
      },
      "source": [
        "def LaBSEembeddings(source, target):\r\n",
        "  '''\r\n",
        "  Generate LABSE embeddings\r\n",
        "  Note: Inputs are array of strings\r\n",
        "  '''           \r\n",
        "  embeddings_input_1 = model.encode(source,show_progress_bar=True)\r\n",
        "  embeddings_input_2 = model.encode(target,show_progress_bar=True)    \r\n",
        "  print(\"LABSE embedding generation finished\")\r\n",
        "  return embeddings_input_1, embeddings_input_2\r\n",
        "  "
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qOlbPq98GCh"
      },
      "source": [
        "def match_target_sentence(source_embedding, target_embeddings):\r\n",
        "  '''\r\n",
        "  Calculate cosine similarity using scipy distance method\r\n",
        "  '''\r\n",
        "  distances = distance.cdist(source_embedding, target_embeddings, \"cosine\")[0]\r\n",
        "  min_index = np.argmin(distances)\r\n",
        "  min_distance = 1 - distances[min_index]\r\n",
        "  # print(\"Match score: {}\".format(min_distance))\r\n",
        "  if min_distance >= 0.5:\r\n",
        "      return min_index, min_distance, \"MATCH\"\r\n",
        "  else:\r\n",
        "      return min_index, min_distance, \"NOMATCH\"     \r\n",
        "      \r\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GceylYunEIAC"
      },
      "source": [
        "def main():\r\n",
        "  source_embeddings, target_embeddings = LaBSEembeddings(so_sentences, ta_sentences)\r\n",
        "  target_map = {}\r\n",
        "  source_map = {}\r\n",
        "  for s in range(len(source_embeddings)):\r\n",
        "    source_map[s] = so_sentences[s]\r\n",
        "  for t in range(len(target_embeddings)):\r\n",
        "    target_map[t] = ta_sentences[t]\r\n",
        "  aligned_phrases= {}\r\n",
        "  for s in range(len(source_embeddings)) :\r\n",
        "      alignments = match_target_sentence([source_embeddings[s]], target_embeddings)\r\n",
        "      if alignments is not None and alignments[2] is \"MATCH\":\r\n",
        "          aligned_phrases[source_map[s]] =  target_map[alignments[0]]\r\n",
        "  return aligned_phrases   \r\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8y5BE3qEmtQ"
      },
      "source": [
        "aligned_phrases = main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL1cwdpuTDzX",
        "outputId": "14fd660b-9086-4ac0-c479-152e27c00626"
      },
      "source": [
        "print(len(aligned_phrases))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQqrf2ohTiI2"
      },
      "source": [
        "ap = []\r\n",
        "for key in aligned_phrases:\r\n",
        "  l = []\r\n",
        "  l.append(key)\r\n",
        "  l.append(aligned_phrases[key])\r\n",
        "  ap.append(l)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0e2rBtXKeoI"
      },
      "source": [
        "import csv\r\n",
        "with open('matched.csv', 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerow(['English', 'Tamil'])\r\n",
        "    for a in ap:\r\n",
        "      writer.writerow(a)"
      ],
      "execution_count": 119,
      "outputs": []
    }
  ]
}