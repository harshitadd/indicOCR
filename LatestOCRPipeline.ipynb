{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LatestOCRPipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8vktgnGRxjbGks3Gw6PUv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/AI4BharatTranslation/blob/main/LatestOCRPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMdaBYIoP6_H"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "from time import sleep\n",
        "import glob\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT1j3nuiP6_I"
      },
      "source": [
        "word_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "google_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "layout_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "segmenter_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "bs_url =\"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/jobs/search/bulk\"\n",
        "evaluator_url  = \"https://auth.anuvaad.org/anuvaad-etl/document-processor/evaluator/v0/process\"\n",
        "download_url =\"https://auth.anuvaad.org/download/\"\n",
        "upload_url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "token = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyTmFtZSI6ImhhcnNoaXRhZGRAZ21haWwuY29tIiwicGFzc3dvcmQiOiJiJyQyYiQxMiRSUVFqQTBhMmJqQjhndkNPZjNVOHBlejBaLk8vdG9ld291dGNnSEY4NWs5NGZGUVJMTzdMeSciLCJleHAiOjE2MTQ5MjA3NjJ9.dCXZRsAzGskd2dSg1u0mfTYEoot4wFI5Sp5M6KMcjMw'\n",
        "\n",
        "headers = {\n",
        "    'auth-token' :token }\n",
        "language = 'detect'\n",
        "file_format = 'PDF'\n",
        "\n",
        "evaluation_level = 'LINE'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npfAt8fpQZCA",
        "outputId": "98982300-fa4f-48a3-bb98-5d33d9e28689"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOEiDE9-P6_I"
      },
      "source": [
        "parent_folder = \"/content/drive/MyDrive/Te/\"\n",
        "source_pdfs = \"/content/drive/MyDrive/Te/\"\n",
        "parsed_pdf_dir = \"/content/drive/MyDrive/Te/parsed/\"\n",
        "text_dir = \"/content/drive/MyDrive/Te/text/\"\n",
        "sentences_dir = \"/content/drive/MyDrive/Te/sent/\"\n",
        "aligned_sentences_path = \"/content/drive/MyDrive/Te/aligned/\"\n",
        "\n",
        "# parent_folder = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/\"\n",
        "# source_pdfs = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/\"\n",
        "# parsed_pdf_dir = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/parsed/\"\n",
        "# text_dir = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/text/\"\n",
        "# sentences_dir = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/sent/\"\n",
        "# aligned_sentences_path = \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/aligned/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVq8hmgtP6_J"
      },
      "source": [
        "def google_ocr(word_url,headers,pdf_name):\n",
        "    \n",
        "    file = {\n",
        "       \"files\": [\n",
        "        {\n",
        "            \"locale\": \"en\",\n",
        "            \"path\": pdf_name,\n",
        "            \"type\": file_format,\n",
        "            \"config\":{\n",
        "        \"OCR\": {\n",
        "          \"option\": \"HIGH_ACCURACY\",\n",
        "          \"language\": language\n",
        "        }\n",
        "        }}\n",
        "    ],\n",
        "    \"workflowCode\": \"WF_A_OGV\"\n",
        "    }\n",
        "    res = requests.post(word_url,json=file,headers=headers)\n",
        "    return res.json()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "082r5MXVP6_J"
      },
      "source": [
        "def upload_file(pdf_file,headers,url):\n",
        "    #url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "    files = [\n",
        "        ('file',(open(pdf_file,'rb')))] \n",
        "\n",
        "    response = requests.post(url, headers=headers, files=files)\n",
        "    \n",
        "    return response.json()\n",
        "    response.json()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myhs2ktDP6_K"
      },
      "source": [
        "def download_file(download_url,headers,outputfile,f_type='json'):\n",
        "    download_url =download_url+str(outputfile)\n",
        "    res = requests.get(download_url,headers=headers)\n",
        "    if f_type == 'json':\n",
        "        return res.json()\n",
        "    else :\n",
        "        return res.content"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc75CcCoP6_K"
      },
      "source": [
        "def save_json(path,res):\n",
        "    with open(path, \"w\", encoding='utf8') as write_file:\n",
        "        json.dump(res, write_file,ensure_ascii=False )\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyiyvcVjP6_K"
      },
      "source": [
        "def bulk_search(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    print(res.json())\n",
        "    prev_progress = \"\"\n",
        "   \n",
        "    while(1):\n",
        "        \n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "       \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']\n",
        "            print(outputfile)\n",
        "            print(job_id)\n",
        "            return outputfile\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        if progress != prev_progress:\n",
        "            print(progress)\n",
        "            prev_progress = progress\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymcVysLkP6_L"
      },
      "source": [
        "def bulk_search_all(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    print(res.json())\n",
        "    \n",
        "   \n",
        "    while(1):\n",
        "        \n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "       \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']\n",
        "            print(outputfile)\n",
        "            print(job_id)\n",
        "            return outputfile\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        print(progress)\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzlipU-ZP6_L"
      },
      "source": [
        "def execute_module(module,url,input_file,module_code,pdf_dir,overwirte=True , draw=True):\n",
        "    \n",
        "        \n",
        "        output_path = os.path.join(pdf_dir,'{}.json'.format(module_code))\n",
        "        if os.path.exists(output_path) and not overwirte:\n",
        "            print(' loading *****************{}'.format(module_code ))\n",
        "            with open(output_path,'r') as wd_file :\n",
        "                response = json.load(wd_file)\n",
        "                \n",
        "            wf_res = pdf_dir + '/{}_wf.json'.format(module_code)\n",
        "            with open(wf_res,'r') as wd_file :\n",
        "                json_file = json.load(wd_file) \n",
        "            job_to_add = (pdf_dir)\n",
        "\n",
        "        else :\n",
        "            if module_code in ['wd','gv']:\n",
        "                res = upload_file(input_file,headers,upload_url)\n",
        "                print('upload response **********', res)\n",
        "                pdf_name = res['data']\n",
        "                response = module(url,headers,pdf_name)\n",
        "            \n",
        "            else : \n",
        "                response = module(url,headers,input_file)\n",
        "                \n",
        "                if 'eval' in module_code :\n",
        "                    json_file = response['outputFile']\n",
        "                    response = download_file(download_url,headers,json_file)\n",
        "                    save_json(output_path,response)\n",
        "                    return json_file,response\n",
        "                \n",
        "            \n",
        "            print(' response *****************{} {}'.format(module_code ,response ))\n",
        "            job_id = response['jobID']\n",
        "            \n",
        "            job_to_add = (job_id,bs_url,headers,pdf_dir,module_code,output_path)\n",
        "        return job_to_add"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJU1VjifP6_M"
      },
      "source": [
        "def evaluate__and_save_input(pdf_files,output_dir,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
        "    word_responses   = {}\n",
        "    layout_responses = {}\n",
        "    segmenter_responses = []\n",
        "    jobs_to_track = []\n",
        "    for pdf in pdf_files:\n",
        "        try :\n",
        "            pdf_name = pdf.split('/')[-1].split('.')[0]\n",
        "            parent_folder_name = pdf.split('/')[-2]\n",
        "            print(pdf , ' is being processed')\n",
        "            pdf_output_dir = os.path.join(output_dir,parent_folder_name,pdf_name)\n",
        "            os.system('mkdir -p \"{}\"'.format(pdf_output_dir))\n",
        "\n",
        "            job_to_add = execute_module(google_ocr,google_url,input_file=pdf,\\\n",
        "                           module_code='gv',pdf_dir=pdf_output_dir,overwirte=False , draw=False)\n",
        "            \n",
        "            jobs_to_track.append(job_to_add)\n",
        "\n",
        "        except Exception as e : \n",
        "            print(e)\n",
        "            logging.error('error in file {}  \\n {}'.format(pdf_name,e))\n",
        "\n",
        " \n",
        "    for job in jobs_to_track:\n",
        "        print(\"----------\")\n",
        "        print(job)\n",
        "        job_id,bs_url,headers,pdf_dir,module_code,output_path = job\n",
        "        json_file = bulk_search(job_id,bs_url,headers)\n",
        "        save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   \n",
        "        print('bulk search  response **************',json_file )\n",
        "        response = download_file(download_url,headers,json_file)\n",
        "        save_json(output_path,response)\n",
        "\n",
        "    print(\"----------------\")\n",
        "    print(jobs_to_track)\n",
        "    print(\"----------------\")\n",
        "    return jobs_to_track"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKvYZUYVP6_N"
      },
      "source": [
        "def main(path,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
        "        pdf_names = glob.glob(path + '/*/*.pdf')[:4]\n",
        "        pdf_names.reverse()\n",
        "        return evaluate__and_save_input(pdf_names,parsed_pdf_dir,headers,word_url,layout_url,download_url,upload_url,bs_url)\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04ptF-t-P6_O",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "outputId": "77aa0dc0-005d-4a92-f87e-838431615d81"
      },
      "source": [
        "main(source_pdfs,headers,word_url,layout_url,download_url,upload_url,bs_url)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Te/11/s11_en.pdf  is being processed\n",
            " loading *****************gv\n",
            "/content/drive/MyDrive/Te/11/s11_te.pdf  is being processed\n",
            " loading *****************gv\n",
            "/content/drive/MyDrive/Te/12/s12_en.pdf  is being processed\n",
            " loading *****************gv\n",
            "/content/drive/MyDrive/Te/12/s12_te.pdf  is being processed\n",
            " loading *****************gv\n",
            "----------\n",
            "/content/drive/MyDrive/Te/parsed/11/s11_en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6f0d9d02a52a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_pdfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayout_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupload_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-d99ebd002b30>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(path, headers, word_url, layout_url, download_url, upload_url, bs_url)\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mpdf_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/*/*.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mpdf_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mevaluate__and_save_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparsed_pdf_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayout_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupload_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-06c802b7fcc1>\u001b[0m in \u001b[0;36mevaluate__and_save_input\u001b[0;34m(pdf_files, output_dir, headers, word_url, layout_url, download_url, upload_url, bs_url)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpdf_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodule_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbulk_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msave_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/{}_wf.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 6)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO4uBcl0P6_P"
      },
      "source": [
        "#### Cleanup failed parses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EGZUQilP6_P"
      },
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "present = defaultdict(lambda :0)\n",
        "for a in glob.glob(parsed_pdf_dir+\"/*/*/*\"):\n",
        "    b = a.split(\"/\")[-3]\n",
        "    present[b] = present[b]+1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WVNpyS1P6_Q"
      },
      "source": [
        "for key,value in present.items():\n",
        "    if value != 4:\n",
        "        print(key)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mrqlrO5P6_Q"
      },
      "source": [
        "folders = []\n",
        "for a in glob.glob(parsed_pdf_dir+\"/*\"):\n",
        "    folders.append(a.split(\"/\")[-1])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3v-wkNRP6_Q",
        "outputId": "f4d10176-1f1f-482f-ba15-05855bf7c2df"
      },
      "source": [
        "set(folders).difference(present.keys())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayzs87mSP6_R"
      },
      "source": [
        "# pdf_names = glob.glob(source_pdfs + '/*/*.pdf')\n",
        "# pds_names_of_intrest = [p for p in pdf_names if any([i in p for i in ['GA_Speech_2018_.',\"Governor's Address 2001-02-Revised-.\",\"Governor's Address 2005-06- .\",\"Budget_speech2018_19.\"]])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM-ncQunP6_R"
      },
      "source": [
        "# pds_names_of_intrest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLF9itpAP6_R"
      },
      "source": [
        "### Json to text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vd57sWCP6_R"
      },
      "source": [
        "def extract_text(parent,folder_name):\n",
        "    with open(parsed_pdf_dir + f\"{parent}/{folder_name}/gv.json\",'r') as f:\n",
        "        data = json.load(f)\n",
        "    pages = data['outputs'][0]['pages']\n",
        "    all_texts = []\n",
        "    for page in pages:\n",
        "        page_text = []\n",
        "        lines = page['lines']\n",
        "        for line in lines:\n",
        "            page_text.append(line['text'])\n",
        "        all_texts.append(page_text)\n",
        "    all_lines = [line for al in all_texts for line in al]\n",
        "    all_text = \" \".join(all_lines)\n",
        "    lang = folder_name[-2:]\n",
        "    os.system('mkdir -p \"{}\"'.format(text_dir + f\"{parent}\"))\n",
        "    with open(text_dir + f\"{parent}/{folder_name}.txt\",'w',encoding='utf-16') as f:\n",
        "        f.writelines(all_text)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjGlSWFHwydm"
      },
      "source": [
        "total_parsed = len(glob.glob(parsed_pdf_dir + '/*/*'))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZi9lOBww3bw",
        "outputId": "938154a5-cf0a-4c34-fc90-baabe8830783"
      },
      "source": [
        "total_parsed"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DAiZk6XP6_R"
      },
      "source": [
        "import glob\n",
        "import shutil\n",
        "\n",
        "from pathlib import Path\n",
        "failed = 0\n",
        "failed_parses = []\n",
        "for name in glob.glob(parsed_pdf_dir + '/*/*'):\n",
        "    folder_name = name.replace(parsed_pdf_dir,\"\")\n",
        "    parent ,  folder_name = folder_name.split(\"/\")\n",
        "    try:\n",
        "        extract_text(parent,folder_name)\n",
        "    except:\n",
        "        print(folder_name)\n",
        "        failed+=1\n",
        "        failed_parses.append(folder_name)\n",
        "      "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myNB88awyvBS",
        "outputId": "53abbe6c-c810-439c-bf4d-bc10e75b64cf"
      },
      "source": [
        "print(failed)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uwDKoEV3k-A"
      },
      "source": [
        "for folder_name in failed_parses:\r\n",
        "    dirpath = Path(text_dir + f\"{folder_name}\")\r\n",
        "    if dirpath.exists():\r\n",
        "        shutil.rmtree(dirpath)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tmVrBBkP6_S"
      },
      "source": [
        "#### Cleanup failed pdf to text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY_KHOYyy7dG",
        "outputId": "6b0965ed-badd-4f03-b23c-64874b41de57"
      },
      "source": [
        "files = len(glob.glob(text_dir+\"/*/*\"))\r\n",
        "print(files)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3ighpi5P6_S"
      },
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "present = defaultdict(lambda :0)\n",
        "for a in glob.glob(text_dir+\"/*/*\"):\n",
        "    b = a.split(\"/\")[-2]\n",
        "    present[b] = present[b]+1\n",
        "\n",
        "to_cleanup = []\n",
        "for key,value in present.items():\n",
        "#     print(key,value)\n",
        "    if value != 2:\n",
        "        to_cleanup.append(key)\n",
        "        print(key)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsURp_l8P6_S"
      },
      "source": [
        "for a in glob.glob(text_dir+\"/*\"):\n",
        "    if any(i in a for i in to_cleanup):\n",
        "        shutil.rmtree(a)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9On4JfwmP6_S"
      },
      "source": [
        "### Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N00gfch2P6_S"
      },
      "source": [
        "def upload_file(file):\n",
        "    url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
        "    files = [('file',(open(file,'rb')))] \n",
        "    response = requests.post(url, headers=headers, files=files)\n",
        "    print(response.json())\n",
        "    return response.json()['data']\n",
        "\n",
        "\n",
        "def submit_tokenize_job(file_path):\n",
        "    body = {\n",
        "            \"workflowCode\": \"WF_A_TOK\",\n",
        "            \"jobName\": \"TOKENIZE_WF_TEST\",\n",
        "            \"files\": [\n",
        "                {\n",
        "                    \"locale\": \"en\",\n",
        "                    \"path\": file_path,\n",
        "                    \"type\": \"txt\"\n",
        "                }]}\n",
        "    url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "    response = requests.post(url, headers=headers,json=body).json()\n",
        "    print(response)\n",
        "    return response['jobID']\n",
        "\n",
        "def bulk_search_tokenize(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"}\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    print(res.json())\n",
        "    prev_progress = \"\"\n",
        "    \n",
        "    while(1):\n",
        "      progress = res.json()['jobs'][0]['status']\n",
        "      print(progress)\n",
        "      if progress in ['COMPLETED','FAILED','FINISHED','SUCCESS']:\n",
        "          data = res.json()\n",
        "          outputfile = res.json()['jobs'][0]['taskDetails'][0]['output'][0]['outputFile']\n",
        "          print(data['jobs'][0]['output'])\n",
        "          return outputfile\n",
        "          break \n",
        "      sleep(0.5)\n",
        "      if progress != prev_progress:\n",
        "          print(progress)\n",
        "          prev_progress = progress\n",
        "      res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwEbC9dVZ7_-"
      },
      "source": [
        "f = glob.glob(text_dir+\"/*/*\")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3M9nZmwZ9jL",
        "outputId": "046a4f9c-9750-477d-838c-d79ae6a2a6e6"
      },
      "source": [
        "print(f)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/Te/text/2/s2_en.txt', '/content/drive/MyDrive/Te/text/2/s2_te.txt', '/content/drive/MyDrive/Te/text/9/s9_en.txt', '/content/drive/MyDrive/Te/text/9/s9_te.txt', '/content/drive/MyDrive/Te/text/8/s8_en.txt', '/content/drive/MyDrive/Te/text/8/s8_te.txt', '/content/drive/MyDrive/Te/text/7/s7_en.txt', '/content/drive/MyDrive/Te/text/7/s7_te.txt', '/content/drive/MyDrive/Te/text/12/s12_en.txt', '/content/drive/MyDrive/Te/text/12/s12_te.txt', '/content/drive/MyDrive/Te/text/11/s11_te.txt', '/content/drive/MyDrive/Te/text/11/s11_en.txt', '/content/drive/MyDrive/Te/text/10/s10_en.txt', '/content/drive/MyDrive/Te/text/10/s10_te.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aFRxob_BbC7u",
        "outputId": "7efdf077-20b5-4951-a469-ccd6efe4e698"
      },
      "source": [
        "text_dir"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Te/text/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LImHtEloa-mp"
      },
      "source": [
        "test_text_dir = '/content/drive/MyDrive/TEST'"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCvlL6TcbY5S",
        "outputId": "58f64c91-4ccf-4df0-9d39-278adf4f75be"
      },
      "source": [
        "glob.glob(test_text_dir+\"/*\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/TEST/Test_en.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Pmvr4KJtS0"
      },
      "source": [
        "folder = '/content/drive/MyDrive/TEST/'"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBeebXhNYolS",
        "outputId": "7f0d03dd-0082-40f0-9668-7163f0ee54bd"
      },
      "source": [
        "jobs_to_track = []\r\n",
        "for file in glob.glob(test_text_dir+\"/*\"):\r\n",
        "    print(file)\r\n",
        "    upload_file(file)\r\n",
        "    job_id = submit_tokenize_job(file)\r\n",
        "    jobs_to_track.append((job_id,folder.split(\"/\")[-1]))\r\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/TEST/Test_en.txt\n",
            "{'data': '083b6d1c-cfcd-4e9f-881c-ed1814c81392.txt', 'http': {'status': 200}, 'ok': True, 'why': 'request successful'}\n",
            "{'active': True, 'input': {'files': [{'locale': 'en', 'path': '/content/drive/MyDrive/TEST/Test_en.txt', 'type': 'txt'}], 'jobName': 'TOKENIZE_WF_TEST', 'workflowCode': 'WF_A_TOK'}, 'jobID': 'A_TK-flIrp-1614877864275', 'metadata': {'module': 'WORKFLOW-MANAGER', 'orgID': 'ANUVAAD', 'receivedAt': 1614877864275, 'requestID': '8590629d-79ba-4905-99f5-3723ae80394c', 'sessionID': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyTmFtZSI6ImhhcnNoaXRhZGRAZ21haWwuY29tIiwicGFzc3dvcmQiOiJiJyQyYiQxMiRSUVFqQTBhMmJqQjhndkNPZjNVOHBlejBaLk8vdG9ld291dGNnSEY4NWs5NGZGUVJMTzdMeSciLCJleHAiOjE2MTQ5MjA3NjJ9.dCXZRsAzGskd2dSg1u0mfTYEoot4wFI5Sp5M6KMcjMw', 'userID': '03e160d36bc34c8fb3a0fbb63e4b02701613729142248'}, 'startTime': 1614877864325, 'state': 'INITIATED', 'status': 'STARTED', 'taskDetails': [], 'workflowCode': 'WF_A_TOK'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R_d36qxan0A"
      },
      "source": [
        "test_sentences_path = \"/content/drive/MyDrive/Te/TEST/sent/\"\r\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ1Kkc0FYT7k"
      },
      "source": [
        "for job_id, folder in jobs_to_track:\r\n",
        "    file_to_download = bulk_search_tokenize(job_id,bs_url,headers)\r\n",
        "    print(file_to_download)\r\n",
        "    for key , value in file_to_download.items():\r\n",
        "        content = download_file(value)\r\n",
        "        os.system('mkdir -p \"{}\"'.format(test_sentences_path+folder))\r\n",
        "        with open(os.path.join(test_sentences_path,folder,f\"{key}.text\"),'w',encoding=\"utf-16\") as f:\r\n",
        "            f.writelines(content.decode('utf-16'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGurarHnYRhO"
      },
      "source": [
        "def download_file(outputfile):\r\n",
        "\r\n",
        "    download_url =\"https://auth.anuvaad.org/download/\"+str(outputfile)\r\n",
        "    res = requests.get(download_url,headers=headers)\r\n",
        "    return res.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GQwmvQyP6_S",
        "scrolled": true
      },
      "source": [
        "failed_files = []\n",
        "for file in glob.glob(text_dir+ \"*/*.txt\"):  \n",
        "    lang = file.split(\".txt\")[0].split(\"_\")[-1]\n",
        "    print(lang)\n",
        "    file_path = upload_file(file)\n",
        "    tokenized_file_id = tokenize_text(file_path,lang)\n",
        "    print('Tokenized file id ', tokenized_file_id)\n",
        "    if tokenized_file_id:\n",
        "        content = download_file(tokenized_file_id)\n",
        "        os.system('mkdir -p \"{}\"'.format(sentences_dir+ file.split(\"/\")[-2]))\n",
        "        write_path = sentences_dir+ file.split(\"/\")[-2] + \"/\" + file.split(\"/\")[-1]\n",
        "        with open(write_path,'w',encoding=\"utf-16\") as f:\n",
        "            f.writelines(content.decode('utf-16'))\n",
        "    else:\n",
        "        failed_files.append((file_path,lang))\n",
        "# failed_files_again = []\n",
        "# for file_path,lang in failed_files:\n",
        "#     tokenized_file_id = tokenize_text(file_path,lang)\n",
        "#     if tokenized_file_id:\n",
        "#         content = download_file(tokenized_file_id)\n",
        "#         os.system('mkdir -p \"{}\"'.format(sentences_dir+ file.split(\"/\")[-2]))\n",
        "#         write_path = sentences_dir+ file.split(\"/\")[-2] + \"/\" + file.split(\"/\")[-1]\n",
        "#         with open(write_path,'w',encoding=\"utf-16\") as f:\n",
        "#             f.writelines(content.decode('utf-16'))\n",
        "#     else:\n",
        "#         failed_files_again.append((file_path,lang))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXWiWOmO2XYq",
        "outputId": "aaf28dd3-3abd-4e8a-b1aa-5a8ed7b007e6"
      },
      "source": [
        "print(len(failed_files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zGTgEPYP6_T"
      },
      "source": [
        "#### Clean up failed setence segmentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMCFC3f4P6_T"
      },
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "present = defaultdict(lambda :0)\n",
        "for a in glob.glob(sentences_dir+\"/*/*\"):\n",
        "    b = a.split(\"/\")[-2]\n",
        "    present[b] = present[b]+1\n",
        "\n",
        "to_cleanup = []\n",
        "for key,value in present.items():\n",
        "#     print(key,value)\n",
        "    if value != 2:\n",
        "        to_cleanup.append(key)\n",
        "        print(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bRbAZv8P6_T"
      },
      "source": [
        "for a in glob.glob(sentences_dir+\"/*\"):\n",
        "    if any(i in a for i in to_cleanup):\n",
        "        shutil.rmtree(a)\n",
        "#         print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDZv_wWP6_T"
      },
      "source": [
        "### Sentence Alignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgd2kTpNP6_T"
      },
      "source": [
        "upload_url = \"https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file\"\n",
        "aligner_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
        "download_url = \"https://auth.anuvaad.org/download/\"\n",
        "search_url = \"'https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/jobs/search/bulk'\"\n",
        "\n",
        "# input_path = \"/Users/eaxxkra/Downloads/tn_budget_sentences/\"\n",
        "# output_path = \"/Users/eaxxkra/Downloads/tn_budget_aligned_sentences/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgKxr-rBP6_T",
        "scrolled": true
      },
      "source": [
        "def bulk_search_alignment(job_id,bs_url,headers):\n",
        "    bs_request = {\n",
        "    \"jobIDs\": [job_id],\n",
        "    \"taskDetails\":\"true\"\n",
        "    }\n",
        "    print(job_id)\n",
        "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "    prev_progress = \"\"\n",
        "   \n",
        "    while(1):\n",
        "\n",
        "        progress = res.json()['jobs'][0]['status']\n",
        "      \n",
        "        if progress in ['COMPLETED','FAILED']:\n",
        "            print(progress)\n",
        "            data = res.json()\n",
        "            secondlanguage = data['jobs'][0]['input']['files'][0]['locale']    \n",
        "            print(secondlanguage)\n",
        "            source , target = \"source\" , \"target\"\n",
        "            if secondlanguage == 'en.txt':\n",
        "                source , target = target, source\n",
        "            nomatch=str(data['jobs'][0]['output']['noMatch']['source'])\n",
        "            match_english=str(data['jobs'][0][\"output\"]['match'][target])\n",
        "            match_non_english=str(data['jobs'][0][\"output\"]['match'][source])\n",
        "            almostmatch_english=str(data['jobs'][0]['output']['almostMatch'][target])\n",
        "            almostatch_non_english=str(data['jobs'][0]['output']['almostMatch'][source])\n",
        "            return {\n",
        "                'match_english' : match_english,\n",
        "                'match_non_english' : match_non_english,\n",
        "                'almost_match_english' : almostmatch_english,\n",
        "                'almost_non_match_english' : almostatch_non_english,\n",
        "                'no_match' : nomatch}\n",
        "            break\n",
        "        sleep(0.5)\n",
        "        if progress != prev_progress:\n",
        "            print(progress)\n",
        "            prev_progress = progress\n",
        "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
        "        #print(res.json())\n",
        "      \n",
        "\n",
        "def upload_files(folder):\n",
        "    path_lang_tup = []\n",
        "    for file in glob.glob(folder+\"/*\"):\n",
        "        print(file)\n",
        "        lang = file.split(\"_\")[-1]        \n",
        "        path_lang_tup.append((upload_file(file),lang))\n",
        "    return path_lang_tup\n",
        "\n",
        "def submit_alignment_job(path_lang_tup):\n",
        "    print(path_lang_tup)\n",
        "    files = []\n",
        "    for path, lang in path_lang_tup:\n",
        "        files.append({\n",
        "                        \"locale\": lang,\n",
        "                        \"path\": path,\n",
        "                        \"type\": \"txt\"\n",
        "                    })\n",
        "    \n",
        "    \n",
        "    aligner_body = {\n",
        "        \"workflowCode\":\"WF_A_AL\",\n",
        "        \"files\": files}\n",
        "    \n",
        "    aligner_response = requests.request(\"POST\", aligner_url, json=aligner_body, headers=headers).json()\n",
        "    return aligner_response['jobID']\n",
        "    \n",
        "jobs_to_track = []\n",
        "for folder in glob.glob(sentences_dir+\"/*\"):\n",
        "    print(folder)\n",
        "    path_lang_tup = upload_files(folder)\n",
        "    job_id = submit_alignment_job(path_lang_tup)\n",
        "    jobs_to_track.append((job_id,folder.split(\"/\")[-1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ0rFzE3P6_U",
        "scrolled": true
      },
      "source": [
        "\n",
        "for job_id, folder in jobs_to_track:\n",
        "    file_to_download = bulk_search_alignment(job_id,bs_url,headers)\n",
        "    for key , value in file_to_download.items():\n",
        "        content = download_file(value)\n",
        "        os.system('mkdir -p \"{}\"'.format(aligned_sentences_path+folder))\n",
        "        with open(os.path.join(aligned_sentences_path,folder,f\"{key}.text\"),'w',encoding=\"utf-16\") as f:\n",
        "            f.writelines(content.decode('utf-16'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twRv1icrP6_U"
      },
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "present = defaultdict(lambda :0)\n",
        "for a in glob.glob(aligned_sentences_path+\"/*/*\"):\n",
        "    b = a.split(\"/\")[-2]\n",
        "    present[b] = present[b]+1\n",
        "\n",
        "to_cleanup = []\n",
        "for key,value in present.items():\n",
        "#     print(key,value)\n",
        "    if value != 5:\n",
        "        to_cleanup.append(key)\n",
        "        print(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKoPg3VwP6_U"
      },
      "source": [
        "to_cleanup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYMkY1btP6_U"
      },
      "source": [
        "### Aligned Sentences to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0CWhibjP6_U"
      },
      "source": [
        "import pandas as pd\n",
        "match_dfs = []\n",
        "almost_match_dfs = []\n",
        "for a in glob.glob(aligned_sentences_path+\"/*\"):\n",
        "    match = pd.DataFrame()\n",
        "    with open(a + \"/\" + \"match_english.text\",encoding='utf-16') as f:\n",
        "        match['english'] = f.readlines()\n",
        "    with open(a + \"/\" + \"match_non_english.text\",encoding='utf-16') as f:\n",
        "        match['non_english'] = f.readlines()\n",
        "    match_dfs.append(match)\n",
        "    almost_match = pd.DataFrame()\n",
        "    with open(a + \"/\" + \"almost_match_english.text\",encoding='utf-16') as f:\n",
        "        almost_match['english'] = f.readlines()\n",
        "    with open(a + \"/\" + \"almost_non_match_english.text\",encoding='utf-16') as f:\n",
        "        almost_match['non_english'] = f.readlines()\n",
        "    match_dfs.append(match)\n",
        "    almost_match_dfs.append(almost_match)\n",
        "\n",
        "match = pd.concat(match_dfs)\n",
        "almost_match = pd.concat(almost_match_dfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A0nsCVFP6_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872ed9ae-eab3-4715-a1be-dd41c793890b"
      },
      "source": [
        "len(match)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1418"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT7UfJFoP6_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67122b63-edd0-45bd-fa1d-74ba29ed626b"
      },
      "source": [
        "len(almost_match)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJNqh6uVP6_V"
      },
      "source": [
        "match.to_csv(parent_folder + \"match.csv\")\n",
        "almost_match.to_csv(parent_folder + \"almost_match.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eESLvse5zj1E"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D26zBJUGP6_V"
      },
      "source": [
        "files = os.listdir(source_pdfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIb0mcNyq6IR",
        "outputId": "eb5a2278-c1a1-4c0e-bf3a-671382ce82f7"
      },
      "source": [
        "print(files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sent', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', 'parsed', 'text', 'aligned', 'match.csv', 'almost_match.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy75Rfc9uYKZ"
      },
      "source": [
        "parallel_docs[0].replace('ta','en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr-P16hcq69S"
      },
      "source": [
        "for file in files:\r\n",
        "  failed = 0\r\n",
        "  try: \r\n",
        "    parallel_docs = os.listdir(source_pdfs+file)\r\n",
        "    if len(parallel_docs)!=2:\r\n",
        "      print('Not Complete')\r\n",
        "      failed+=1\r\n",
        "    else:\r\n",
        "      if 'en' in parallel_docs[0]:\r\n",
        "        n = parallel_docs[0].replace('ta','en')\r\n",
        "        nn = source_pdfs + n \r\n",
        "        src = source_pdfs + '/' + file + '/' + parallel_docs[0]\r\n",
        "        os.rename(src,nn)\r\n",
        "      if 'en' in parallel_docs[1]:\r\n",
        "        n = parallel_docs[1].replace('ta','en')\r\n",
        "        nn = source_pdfs + '/' + file + '/' + n\r\n",
        "        src = source_pdfs + parallel_docs[1]\r\n",
        "        os.rename(src,nn)\r\n",
        "  except:\r\n",
        "    print(file + ' is not a Directory ?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxfFtIhO2WV_"
      },
      "source": [
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT9sfBgsr1NZ"
      },
      "source": [
        "files = os.listdir(source_pdfs)\r\n",
        "for file in files:\r\n",
        "  if '.pdf' in file:\r\n",
        "    print(file)\r\n",
        "    dest_folder = file.split('_')[1]\r\n",
        "    print(\"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/\" + file , \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/\" + dest_folder + '/')\r\n",
        "    shutil.move(\"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/\" + file , \"/content/drive/MyDrive/Ta/Ta_Source_Departmental_Documents/\" + dest_folder + '/' )\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPrdJyiV2HyS"
      },
      "source": [
        "files = os.listdir(source_pdfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAVx1xzJ2vyT",
        "outputId": "b14b6069-ea7c-4cb3-cda6-e53fabd3935b"
      },
      "source": [
        "print(files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sent', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', 'parsed', 'text', 'aligned', 'match.csv', 'almost_match.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ-m-STZ2xFk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}